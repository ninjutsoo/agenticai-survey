<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.3" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">information</journal-id>
      <journal-title-group>
        <journal-title>Information</journal-title>
        <abbrev-journal-title abbrev-type="publisher">Information</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="pubmed">Information</abbrev-journal-title>
      </journal-title-group>
      <issn pub-type="epub">2078-2489</issn>
      <publisher>
        <publisher-name>MDPI</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="doi">10.3390/info16100894</article-id>
      <article-id pub-id-type="publisher-id">information-16-00894</article-id>
      <article-categories>
        <subj-group>
          <subject>Review</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Large Language Model Agents for Biomedicine: A Comprehensive Review of Methods, Evaluations, Challenges, and Future Directions</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-5587-7429</contrib-id>
          <name>
            <surname>Xu</surname>
            <given-names>Xiaoran</given-names>
          </name>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2206-6767</contrib-id>
          <name>
            <surname>Sankar</surname>
            <given-names>Ravi</given-names>
          </name>
          <xref rid="c1-information-16-00894" ref-type="corresp">*</xref>
        </contrib>
      </contrib-group>
      <contrib-group>
        <contrib contrib-type="editor">
          <name>
            <surname>Fujita</surname>
            <given-names>Katsuhide</given-names>
          </name>
          <role>Academic Editor</role>
        </contrib>
      </contrib-group>
      <aff id="af1-information-16-00894">Department of Electrical Engineering, University of South Florida, Tampa, FL 33620, USA; <email>xiaoranxu@usf.edu</email></aff>
      <author-notes>
        <corresp id="c1-information-16-00894"><label>*</label>Correspondence: <email>sankar@usf.edu</email></corresp>
      </author-notes>
      <pub-date pub-type="epub">
        <day>14</day>
        <month>10</month>
        <year>2025</year>
      </pub-date>
      <pub-date pub-type="collection">
        <month>10</month>
        <year>2025</year>
      </pub-date>
      <volume>16</volume>
      <issue>10</issue>
      <elocation-id>894</elocation-id>
      <history>
        <date date-type="received">
          <day>17</day>
          <month>09</month>
          <year>2025</year>
        </date>
        <date date-type="rev-recd">
          <day>10</day>
          <month>10</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>12</day>
          <month>10</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; 2025 by the authors.</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access">
          <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Large language model (LLM)-based agents are rapidly emerging as transformative tools across biomedical research and clinical applications. By integrating reasoning, planning, memory, and tool use capabilities, these agents go beyond static language models to operate autonomously or collaboratively within complex healthcare settings. This review provides a comprehensive survey of biomedical LLM agents, spanning their core system architectures, enabling methodologies, and real-world use cases such as clinical decision making, biomedical research automation, and patient simulation. We further examine emerging benchmarks designed to evaluate agent performance under dynamic, interactive, and multimodal conditions. In addition, we systematically analyze key challenges, including hallucinations, interpretability, tool reliability, data bias, and regulatory gaps, and discuss corresponding mitigation strategies. Finally, we outline future directions in areas such as continual learning, federated adaptation, robust multi-agent coordination, and human AI collaboration. This review aims to establish a foundational understanding of biomedical LLM agents and provide a forward-looking roadmap for building trustworthy, reliable, and clinically deployable intelligent systems.</p>
      </abstract>
      <kwd-group>
        <kwd>large language models</kwd>
        <kwd>biomedical agents</kwd>
        <kwd>tool-augmented reasoning</kwd>
        <kwd>multi-agent systems</kwd>
        <kwd>trustworthiness AI</kwd>
      </kwd-group>
      <funding-group>
        <funding-statement>This research received no external funding.</funding-statement>
      </funding-group>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro" id="sec1-information-16-00894">
      <title>1. Introduction</title>
      <p>Large language models (LLMs), built on Transformer architectures and pre-trained on massive text corpora [<xref ref-type="bibr" rid="B1-information-16-00894">1</xref>], have achieved remarkable success in natural language understanding and generation tasks [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>]. In recent years, their role has evolved beyond static text generation to encompass interactive reasoning, planning, and tool use core capabilities that characterize intelligent agentic systems [<xref ref-type="bibr" rid="B3-information-16-00894">3</xref>,<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>]. This shift has led to the emergence of LLM-driven agents, capable of acting autonomously or semi-autonomously to complete complex, multi-step tasks.</p>
      <p>In the biomedical domain, this transformation is particularly promising. LLM agents are now being explored for applications such as clinical decision support, scientific literature analysis, drug discovery, and workflow automation [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>,<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>]. Unlike traditional models that simply output responses, biomedical LLM agents operate with enhanced autonomy: they can invoke external tools (e.g., medical calculators, retrieval APIs), maintain contextual memory across interactions, and execute goal-directed behaviors in high-stakes environments [<xref ref-type="bibr" rid="B3-information-16-00894">3</xref>,<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>]. For example, agents like SourceCheckup [<xref ref-type="bibr" rid="B5-information-16-00894">5</xref>] and CalcQA [<xref ref-type="bibr" rid="B6-information-16-00894">6</xref>] have demonstrated capabilities in source-grounded reasoning and numerical tool usage, while MedJourney [<xref ref-type="bibr" rid="B7-information-16-00894">7</xref>] and AgentClinic [<xref ref-type="bibr" rid="B8-information-16-00894">8</xref>] explore collaborative and multi-agent workflows in virtual clinical scenarios.</p>
      <p>We define the biomedical LLM agent as an artificial intelligence system that leverages a large language model as its core cognitive engine, and is equipped with perception, planning, memory, and actuation modules to solve complex problems within biomedical and clinical contexts [<xref ref-type="bibr" rid="B3-information-16-00894">3</xref>,<xref ref-type="bibr" rid="B9-information-16-00894">9</xref>,<xref ref-type="bibr" rid="B10-information-16-00894">10</xref>,<xref ref-type="bibr" rid="B11-information-16-00894">11</xref>]. These agents may support or automate processes such as diagnosis, literature synthesis, physician patient interaction, and trial matching, and are increasingly viewed as foundational building blocks for next-generation clinical AI systems.</p>
      <p>However, the deployment of such agents in the biomedical domain introduces a unique set of challenges. Given the high-stakes nature of medicine, hallucinations, i.e., the generation of plausible but inaccurate or fabricated information, pose substantial risks [<xref ref-type="bibr" rid="B12-information-16-00894">12</xref>]. Furthermore, biomedical AI systems must meet stringent requirements for data privacy (e.g., HIPAA, GDPR), algorithmic fairness, and interpretability, all while conforming to evolving regulatory frameworks such as FDA guidelines and the EU AI Act [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>]. These domain-specific constraints necessitate new design principles, evaluation metrics, and deployment safeguards that go well beyond those used in general purpose NLP systems.</p>
      <p>Despite these advances, there remains a lack of comprehensive surveys examining LLM agents of biomedical and clinical settings (systems with agentic reasoning, memory, tool orchestration, and planning). This paper addresses this gap by providing a focused review of the methods, evaluations, and deployment issues associated with biomedical LLM agents.</p>
      <p>Compared with prior surveys focusing on general medical LLMs or conversational agents [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>,<xref ref-type="bibr" rid="B12-information-16-00894">12</xref>], our review is distinguished by its agent-centric framing. Rather than treating biomedical LLMs as static models, we conceptualize them as agentic systems with capabilities in reasoning, planning, memory, and tool orchestration. We introduce a multi-layer taxonomy spanning (1) core agentic capabilities, (2) enabling methodologies such as retrieval-augmented generation, fine-tuning, and multimodal integration, and (3) biomedical applications and evaluation frameworks. This perspective highlights how traditional NLP systems are evolving toward autonomous, tool-using biomedical agents, providing a unified conceptual and methodological lens not covered in earlier surveys.</p>
      <p>Specifically, we review recent progress in biomedical LLM agents published from 2023 through July 2025, emphasizing agent-centric design patterns, enabling techniques, and domain-specific adaptations. While earlier work on biomedical NLP and knowledge-driven systems laid important groundwork, large-scale agentic applications of LLMs only began to emerge after 2023 with the advent of more capable foundation models and tool-use frameworks. We therefore focus on this recent period, during which the field has rapidly evolved [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>]. We analyze challenges such as hallucination, interpretability, tool reliability, data bias, and regulatory compliance, and explore mitigation strategies proposed in current literature. Finally, we identify promising directions for future development.</p>
      <p>To frame the scope of this review, we begin by summarizing recent agentic advances and systematically categorizing core design patterns, applications, and challenges. The structure of this review is as follows. <xref ref-type="sec" rid="sec3-information-16-00894">Section 3</xref> examines core methodologies and system architectures, including single- and multi-agent designs, tool integration, fine-tuning, and multimodal reasoning. <xref ref-type="sec" rid="sec4-information-16-00894">Section 4</xref> discusses evaluation and benchmarking strategies. <xref ref-type="sec" rid="sec5-information-16-00894">Section 5</xref> reviews key challenges and mitigation techniques. <xref ref-type="sec" rid="sec6-information-16-00894">Section 6</xref> outlines future research directions. <xref ref-type="sec" rid="sec7-information-16-00894">Section 7</xref> concludes this review.</p>
      <p>Accordingly, we pose the following research questions:<list list-type="order"><list-item><label>(1)</label><p>What are the main LLM agent technical paradigms in the biomedical field, and how do they satisfy requirements for subject-matter expertise, interpretability, and regulatory compliance?</p></list-item><list-item><label>(2)</label><p>In clinical and research settings, how can we comprehensively assess LLM agent performance, reliability, and safety using both automated metrics and user studies?</p></list-item><list-item><label>(3)</label><p>What are the key challenges currently faced by biomedical LLM agents in terms of knowledge updating, reasoning interpretability, resource constraints, data privacy, and ethical compliance?</p></list-item></list></p>
    </sec>
    <sec id="sec2-information-16-00894">
      <title>2. Data Preparation</title>
      <p>Having outlined the motivation and scope of this survey, we now describe the literature collection methodology that grounds our analysis. This review is grounded in a structured and comprehensive literature search spanning multiple authoritative sources, including PubMed, arXiv, bioRxiv, medRxiv, and supplementary searches via Google Scholar. The initial search strategy involved various keyword combinations such as &#x201C;large language model agents&#x201D;, &#x201C;agentic AI&#x201D;, and &#x201C;autonomous research&#x201D;, intersected with biomedical-specific terms including &#x201C;drug discovery&#x201D;, &#x201C;clinical diagnostics&#x201D;, &#x201C;genomics&#x201D;, &#x201C;personalized medicine&#x201D;, and &#x201C;biomedicine&#x201D;. The search covered publications from 2023 to June 2025, aligning with the timeline of major developments in biomedical research of LLM agents.</p>
      <p>It is worth noting that prior to 2023, seminal work in biomedical NLP and knowledge-driven AI laid the groundwork for current progress. Early domain-specific models such as BioBERT [<xref ref-type="bibr" rid="B14-information-16-00894">14</xref>] and PubMedBERT [<xref ref-type="bibr" rid="B15-information-16-00894">15</xref>] were pre-trained on biomedical corpora to enhance text understanding tasks, such as entity recognition, relation extraction, and question answering, and demonstrated the value of domain adaptation and structured knowledge integration. Foundational efforts in knowledge-driven discovery, such as Swanson&#x2019;s [<xref ref-type="bibr" rid="B16-information-16-00894">16</xref>] hypothesis generation framework in the late 1980s and its later extension with biomedical knowledge graphs in the 2010s [<xref ref-type="bibr" rid="B17-information-16-00894">17</xref>], further illustrated how structured reasoning could aid hypothesis formation in biomedicine. However, these systems and models were largely static and lacked the general reasoning, interactivity, and tool-use capabilities that characterize agentic behavior. In contrast, foundation models developed after 2023, including GPT-4, Claude 3.5, and Gemini 1.5 Pro, extend beyond text understanding to support multimodal reasoning, long-context comprehension, and function calling. These advances, combined with instruction-following and reflective mechanisms, have transformed modern LLMs into flexible cognitive backbones capable of autonomous planning and collaboration. With the emergence of such foundation models beginning with GPT-3 in 2020 and accelerated by ChatGPT in late 2022 [<xref ref-type="bibr" rid="B18-information-16-00894">18</xref>], agent-centric biomedical applications have rapidly expanded, motivating our review focus on the 2023&#x2013;2025 period. A multistage screening procedure was implemented. In the first stage, non-scholarly sources such as news articles, blog posts, and meta-reviews were excluded. In the second stage, articles were retained based on direct relevance to biomedical LLM agents exhibiting core agentic traits, such as planning, memory, interaction, and tool utilization, and demonstrating practical application in clinical or biomedical research con- texts. Priority was given to original research and technically detailed works, particularly those offering empirical evaluation or system-level contributions.</p>
      <p>Rather than aiming for exhaustive coverage, this review focuses on a representative and high-quality subset of publications that collectively reflect the state of the art and emerging trends in the development and deployment of biomedical LLM agents. The overall search and screening process is outlined in <xref ref-type="fig" rid="information-16-00894-f001">Figure 1</xref>.</p>
    </sec>
    <sec sec-type="methods" id="sec3-information-16-00894">
      <title>3. Methodology and Architecture of Biomedical LLM Agents</title>
      <p>With the foundational literature identified, we next examine the technical foundations that enable agentic behavior in biomedical domains.</p>
      <sec id="sec3dot1-information-16-00894">
        <title>3.1. Fundamental Concepts: From LLMs to Agents</title>
        <p>LLMs serve as powerful natural language processing tools and form the foundation of agents [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>]. However, achieving true agentic behavior requires more than the LLM alone. In the agent paradigm, the LLM acts as the core &#x201C;brain,&#x201D; while additional capability modules augment its functionality. First, perception enables the agent to interpret environmental inputs, ranging from textual and visual data to sensor readings, thus grounding its subsequent reasoning [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>]. Planning mechanisms allow the agent to devise multi-step strategies, dynamically selecting actions that align with its objectives and adapting to the current state of its surroundings [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>,<xref ref-type="bibr" rid="B19-information-16-00894">19</xref>]. Memory modules further enhance performance by maintaining both short-term context for ongoing tasks and long-term repositories of accumulated knowledge and experience [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>]. Finally, action or tool-use capabilities empower the agent to carry out its plans, whether by invoking external services (such as APIs, databases, or code interpreters) or by executing physical operations in embodied scenarios [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>]. Many such agents are built upon broader foundation models, which acquire cross-task generalization capabilities through self-supervised learning on massive datasets [<xref ref-type="bibr" rid="B20-information-16-00894">20</xref>]. Through the integration of these components, LLM agents transcend passive response generation, proactively engaging with their environment to achieve complex, goal-driven outcomes. To systematically organize the architectural space of biomedical agents, we present a unified conceptual landscape in <xref ref-type="fig" rid="information-16-00894-f002">Figure 2</xref>, which outlines the multi-level structure spanning core agentic capabilities, system architectures, enabling techniques, and application scenarios. The agent core layer defines fundamental properties such as goal-driven behavior, interactivity, adaptability, and single- versus multi-agent paradigms; the key methodologies layer encompasses enabling techniques including multimodal integration across text, images, and genomic data, domain- specific fine-tuning, retrieval-augmented generation, and tool use; and the application layer illustrates downstream biomedical use cases ranging from clinical decision support and drug discovery to team collaboration and hybrid scenarios. Cross-cutting challenges such as data quality, catastrophic forgetting, and hallucination, together with performance-enhancement strategies, are also highlighted. Collectively, the figure provides a system-level view that links agentic design principles, methodological enablers, and biomedical applications. <xref ref-type="table" rid="information-16-00894-t001">Table 1</xref> lists representative biomedical LLM agents and their core components.</p>
      </sec>
      <sec id="sec3dot2-information-16-00894">
        <title>3.2. LLM Agent Architecture</title>
        <p>According to the number of agents and the mode of collaboration, biomedical LLM agent systems can be roughly divided into single-agent systems and multi-agent systems (MAS).</p>
        <sec id="sec3dot2dot1-information-16-00894">
          <title>3.2.1. Single-Agent Systems</title>
          <p>Single-agent biomedical LLM agents aim to enhance the intelligence, autonomy, and contextual reasoning capabilities of a single foundational model through several tightly integrated mechanisms.</p>
          <p>First, advanced prompting strategies, such as Chain-of-Thought (CoT) [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>] guides the model to reason step by step through explicit intermediate logic, ReAct (Reasoning and Acting) [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>] combines reasoning with external actions such as tool use or API calls to improve factual grounding, and Tree-of-Thought (ToT) [<xref ref-type="bibr" rid="B34-information-16-00894">34</xref>] expands this idea by exploring multiple reasoning branches hierarchically before selecting the optimal path, are employed to guide the LLM in decomposing complex tasks into structured subgoals. These methods enable the agent to formulate multi-step plans and execute them in a coherent and interpretable manner, facilitating logical consistency and outcome alignment in biomedical scenarios [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>].</p>
          <p>To mitigate the limitations of the context window inherent in most LLMs, memory integration has become a core component. Short-term memory modules are used to maintain active dialogue or task states, while long-term memory allows for the retrieval of accumulated domain knowledge [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>]. In many systems, retrieval-augmented generation (RAG) frameworks are incorporated to dynamically access external sources, such as biomedical literature or patient records, further enriching the model&#x2019;s memory with real-time, factual information [<xref ref-type="bibr" rid="B35-information-16-00894">35</xref>]. Another key functionality is reflection and self-correction. Biomedical LLM agents are increasingly equipped with mechanisms to assess their own outputs and revise suboptimal responses based on internal or external feedback [<xref ref-type="bibr" rid="B35-information-16-00894">35</xref>]. This process may involve internal self-evaluation or external validation via tools or other agents. For instance, BioDiscovery Agent employs a critic agent an auxiliary LLM to review and challenge its reasoning and conclusions [<xref ref-type="bibr" rid="B26-information-16-00894">26</xref>]. Similarly, CRISPR-GPT uses state machines to structure agent behavior and supports multiple rounds of interaction to refine outputs in gene editing design tasks [<xref ref-type="bibr" rid="B25-information-16-00894">25</xref>]. Together, these strategies enable single-agent systems to exhibit more robust, interpretable, and error-tolerant behavior in biomedical applications.</p>
        </sec>
        <sec id="sec3dot2dot2-information-16-00894">
          <title>3.2.2. Multi-Agent Systems (MAS)</title>
          <p>While single-agent systems emphasize internal consistency and autonomy, more complex biomedical tasks often require collaborative intelligence. MAS leverages the collective capabilities of multiple LLM agents to collaboratively solve complex biomedical tasks that are beyond the scope of a single agent. This paradigm is inspired by real-world team-based decision-making processes commonly seen in clinical practice and biomedical research.</p>
          <p>Collaboration among agents in MAS can be orchestrated through different architectural paradigms. In centralized configurations, a central coordinator or trainer agent is responsible for decomposing tasks, assigning subtasks, and managing the communication flow [<xref ref-type="bibr" rid="B36-information-16-00894">36</xref>]. For instance, DDO divides the diagnostic workflow into two specialized agents, one for symptom inquiry and another for disease diagnosis, who collaborate through structured dialogue to emulate physician-patient interactions [<xref ref-type="bibr" rid="B37-information-16-00894">37</xref>]. SM-MAS further demonstrates a scalable and modular MAS framework tailored for adaptive clinical decision-making [<xref ref-type="bibr" rid="B38-information-16-00894">38</xref>]. In addition, CPDE (Centralized Planning with Decentralized Execution) describes systems in which a central controller plans the overall workflow, while local agents execute tasks independently; conversely, DPDE (Decentralized Planning and Decentralized Execution) allows each agent to plan and act autonomously, improving scalability at the cost of coordination complexity. It enables dynamically composed agent teams (e.g., symptom extractors, reasoning agents, verifiers) to interact under a shared protocol, leading to improved accuracy and transparency compared to single-agent baselines.</p>
          <p>Decentralized MAS architectures, by contrast, promote autonomous interaction among agents without centralized control. These can be further categorized into revision-based optimization (where agents iteratively refine each other&#x2019;s outputs) and protocol-based communication (where information is shared according to pre-defined rules) [<xref ref-type="bibr" rid="B39-information-16-00894">39</xref>]. In practice, hybrid systems that blend centralized planning with decentralized execution have demonstrated superior flexibility and scalability, particularly in complex, multi-step biomedical scenarios [<xref ref-type="bibr" rid="B40-information-16-00894">40</xref>]. Planning strategies in MAS typically fall into two main categories: centralized planning with decentralized execution (CPDE), and fully decentralized planning and execution (DPDE), each offering different trade-offs between control and autonomy [<xref ref-type="bibr" rid="B41-information-16-00894">41</xref>]. Communication between agents is facilitated either explicitly via structured message passing or implicitly, through shared memory or state representations [<xref ref-type="bibr" rid="B41-information-16-00894">41</xref>]. Efficient communication protocols, mediator agents, and verification mechanisms are often incorporated to enhance cooperation quality and minimize the propagation of hallucinations or inconsistencies.</p>
          <p>Overall, multi-agent biomedical systems introduce a powerful abstraction for simulating collaborative decision-making, distributing cognitive load, and improving robustness through agent specialization. These frameworks are particularly valuable for tasks requiring multiple expert perspectives, such as rare disease diagnosis, clinical trial matching, and cross-modal reasoning.</p>
        </sec>
      </sec>
      <sec sec-type="methods" id="sec3dot3-information-16-00894">
        <title>3.3. Methodology of Biomedical Agents</title>
        <p>Beyond agent architecture, enabling techniques such as re-training, fine-tuning, tool use, and multimodal processing further define agent functionality. To enable LLM agents to work effectively and reliably in the biomedical field, researchers have adopted various key technologies to enhance their capabilities and overcome inherent limitations.</p>
        <sec id="sec3dot3dot1-information-16-00894">
          <title>3.3.1. Retrieval-Augmented Generation</title>
          <p>RAG is a foundational technique that enhances the factual grounding and domain specificity of LLM agents by integrating them with external knowledge sources [<xref ref-type="bibr" rid="B42-information-16-00894">42</xref>]. This approach addresses two core limitations of general-purpose LLMs: outdated knowledge due to static pretraining and insufficient coverage of biomedical domain-specific concepts. By retrieving relevant and up-to-date information such as PubMed abstracts, clinical guidelines, electronic health records (EHRs), medical knowledge graphs, or real-time web content RAG empowers agents to produce contextually accurate and verifiable responses [<xref ref-type="bibr" rid="B35-information-16-00894">35</xref>].</p>
          <p>The typical RAG pipeline involves several stages: query rewriting, document preprocessing (e.g., chunking), indexing (e.g., embedding-based retrieval or maximal marginal relevance ranking), retrieval, and response generation. Prompt engineering and structured integration of retrieved content are also critical to ensure coherence and factual alignment during response synthesis [<xref ref-type="bibr" rid="B42-information-16-00894">42</xref>].</p>
          <p>Biomedical agents often rely on RAG not only for evidence retrieval but also for tool integration. For instance, SourceCheckup employs RAG to trace whether LLM-generated statements are supported by citations, enabling citation-level fact-checking in medical QA systems [<xref ref-type="bibr" rid="B5-information-16-00894">5</xref>]. Multi-agent systems further demonstrate the power of division of labor in retrieval workflows. In Fan et al.&#x2019;s multi-agent normalization framework [<xref ref-type="bibr" rid="B10-information-16-00894">10</xref>], specialized agents collaboratively perform retrieval, evidence expansion, and decision-making for biomedical entity linking tasks. Overall, RAG provides a scalable mechanism for reducing hallucination, enhancing trustworthiness, and ensuring citation traceability in biomedical LLM agents.</p>
        </sec>
        <sec id="sec3dot3dot2-information-16-00894">
          <title>3.3.2. Fine-Tuning and Domain Adaptation</title>
          <p>While retrieval expands an agent&#x2019;s knowledge access, fine-tuning customizes its core behavior for domain-specific reasoning. Although general-purpose LLMs demonstrate impressive linguistic capabilities, their performance in biomedical contexts is often hindered by limited domain-specific knowledge and contextual understanding [<xref ref-type="bibr" rid="B11-information-16-00894">11</xref>]. Fine-tuning addresses this gap by adapting pre-trained models to biomedical tasks through additional supervised or instruction-based training on curated corpora such as PubMed abstracts, clinical notes, or medical textbooks [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>]. This process enhances the model&#x2019;s grasp of clinical terminology, biomedical reasoning patterns, and task-specific discourse.</p>
          <p>Two main fine-tuning strategies are commonly employed. Full-parameter fine-tuning adjusts all weights of the model, typically requiring significant computational resources and large labeled datasets. In contrast, parameter-efficient fine-tuning (PEFT) methods such as adapters, LoRA, or pre-fix tuning update only a small subset of parameters, offering cost-effective alternatives for institutions with limited resources [<xref ref-type="bibr" rid="B43-information-16-00894">43</xref>]. For example, Taiyi is a bilingual biomedical LLM trained using a two-stage fine-tuning approach that distinguishes generative from discriminative tasks across over 140 Chinese and English datasets [<xref ref-type="bibr" rid="B30-information-16-00894">30</xref>].</p>
          <p>Fine-tuning is often combined with retrieval techniques to improve robustness. MedBioLM integrates supervised fine-tuning and RAG to support question answering across diverse biomedical topics [<xref ref-type="bibr" rid="B31-information-16-00894">31</xref>]. However, fine-tuning also presents several challenges. The most prominent include the scarcity of high-quality annotated biomedical data and the risk of catastrophic forgetting, whereby the model loses previously acquired general knowledge during task-specific training [<xref ref-type="bibr" rid="B44-information-16-00894">44</xref>]. Addressing these issues requires continual learning strategies, domain-aware data curation, and hybrid training pipelines that preserve generalization capabilities while specializing in biomedical semantics.</p>
        </sec>
        <sec id="sec3dot3dot3-information-16-00894">
          <title>3.3.3. Tool Use</title>
          <p>Complementary to static knowledge adaptation, tool use empowers agents to interact with external systems and perform grounded actions. Despite their impressive linguistic and reasoning abilities, LLMs exhibit significant limitations in performing precise calculations, accessing up-to-date information, or interacting with external systems [<xref ref-type="bibr" rid="B6-information-16-00894">6</xref>]. To address these gaps, tool use and function-calling capabilities have been integrated into biomedical LLM agents, enabling them to invoke APIs, access structured databases, execute code, or interface with domain-specific software modules [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>]. These interactions allow agents to go beyond text-only responses and operate as interactive systems capable of performing verifiable, real-world tasks.</p>
          <p>One prominent application is in clinical computation, where models are tasked with executing medical scoring systems or risk prediction algorithms. While LLMs can reason about medical concepts, their arithmetic reliability is often poor. Tools such as OpenMedCalc, code interpreters, and mathematical plugins have been shown to drastically reduce error rates [<xref ref-type="bibr" rid="B45-information-16-00894">45</xref>]. For instance, Goodell et al. demonstrated that integrating external calculation tools with LLMs reduced the error rate by 5.5 times for LLaMA and 13 times for GPT models [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>]. The CalcQA framework further improves execution fidelity through flexible tool orchestration and unit-aware conversions [<xref ref-type="bibr" rid="B6-information-16-00894">6</xref>]. Building on this, ReflecTool adds a self-reflection module that guides tool use based on past outcomes, improving reliability across repeated clinical tasks [<xref ref-type="bibr" rid="B46-information-16-00894">46</xref>]. MedOrch further enhances orchestration across iterative planning rounds and pipeline components [<xref ref-type="bibr" rid="B47-information-16-00894">47</xref>], while MMedAgent introduces modular tool invocation tailored for specialized biomedical tasks [<xref ref-type="bibr" rid="B48-information-16-00894">48</xref>].</p>
          <p>Beyond numerical reasoning, biomedical agents employ tool calling in tasks such as literature and gene retrieval via PubMed or NCBI APIs [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>,<xref ref-type="bibr" rid="B26-information-16-00894">26</xref>], chemical structure analysis via cheminformatics packages [<xref ref-type="bibr" rid="B20-information-16-00894">20</xref>], and automated experimentation through lab interface protocols, or interaction with structured EHR databases [<xref ref-type="bibr" rid="B49-information-16-00894">49</xref>]. In pharmacovigilance, for example, agents use specialized toolchains to detect adverse drug events by combining retrieval, extraction, and interpretation modules [<xref ref-type="bibr" rid="B50-information-16-00894">50</xref>]. Frameworks like LangChain and AutoGen facilitate modular integration of these tools, allowing agents to compose workflows dynamically and respond adaptively to task-specific requirements [<xref ref-type="bibr" rid="B35-information-16-00894">35</xref>].</p>
          <p>Tool-augmented agents bridge the gap between LLM reasoning and biomedical task execution, allowing for grounded, auditable, and real-time interactions with the physical and digital healthcare environment.</p>
        </sec>
        <sec id="sec3dot3dot4-information-16-00894">
          <title>3.3.4. Multimodal Integration</title>
          <p>In real-world settings, biomedical reasoning must span text, imaging, EHR, and genomic data, necessitating multimodal integration. Biomedical data is inherently multimodal, encompassing clinical narratives, imaging (e.g., X-rays, CT, MRI), structured data from electronic health records (EHR), genomic sequences, and physiological signals [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>]. To effectively interpret and reason across these diverse modalities, LLM-based agents increasingly integrate multimodal learning techniques and architectures.</p>
          <p>Two primary architectural paradigms have emerged. One is the CLIP-style model, which uses contrastive learning to align images and text into a shared representation space, enabling efficient retrieval and classification across modalities [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>]. The other is LLM-centric, where non-text modalities are encoded by specialized encoders (e.g., visual or molecular encoders) and projected into the language model&#x2019;s embedding space, allowing agents to perform unified reasoning over multimodal inputs. Representative systems including LLaVA-Med [<xref ref-type="bibr" rid="B52-information-16-00894">52</xref>] and GeneVerse [<xref ref-type="bibr" rid="B53-information-16-00894">53</xref>] enable agents to interpret imaging and genetic data through joint embedding and instruction-following capabilities.</p>
          <p>MedChat further demonstrates this integration by coordinating vision agents and text-based agents under a central controller to support cross-modal diagnostic reasoning [<xref ref-type="bibr" rid="B54-information-16-00894">54</xref>]. Building upon such simulated frameworks, several multimodal biomedical agents have also been validated using real-world clinical datasets, highlighting their practical feasibility. For instance, Ferber et al. [<xref ref-type="bibr" rid="B55-information-16-00894">55</xref>] presented a deployed oncology decision-support agent that combined GPT-4 with MedSAM for image segmentation, a transformer for pathology analysis, and OncoKB for knowledge grounding, achieving 91% diagnostic accuracy on real patient cases. Similarly, AgentClinic [<xref ref-type="bibr" rid="B8-information-16-00894">8</xref>] integrates multimodal EHR, imaging, and dialogue data from the MIMIC-IV dataset to evaluate agents in realistic clinical simulations, while TxAgent [<xref ref-type="bibr" rid="B56-information-16-00894">56</xref>] was assessed on clinical guideline corpora and therapy recommendation tasks using actual drug&#x2013;disease treatment records. Collectively, these evaluations demonstrate that multimodal biomedical agents are progressing from conceptual designs toward clinically validated, deployable systems.</p>
          <p>Beyond clinical diagnostics, multimodal integration is crucial in biomedical research. For instance, cheminformatics tools are used to process molecular structures for drug screening [<xref ref-type="bibr" rid="B20-information-16-00894">20</xref>], and multimodal agents are applied in pharmacovigilance to detect adverse drug events (ADEs) through the interaction of retrieval, extraction, and interpretation modules [<xref ref-type="bibr" rid="B50-information-16-00894">50</xref>]. In drug discovery and genomics, agents like those developed by Liu et al. [<xref ref-type="bibr" rid="B57-information-16-00894">57</xref>,<xref ref-type="bibr" rid="B58-information-16-00894">58</xref>] and Xu et al. [<xref ref-type="bibr" rid="B59-information-16-00894">59</xref>] process graph-structured molecular data, literature, and omics features simultaneously.</p>
          <p>Multimodal capabilities are also essential in task pipelines involving tool chaining and hierarchical planning. For example, TxAgent [<xref ref-type="bibr" rid="B56-information-16-00894">56</xref>] enables multi-step treatment recommendation by integrating medical record analysis, clinical guideline retrieval, drug database querying, and executable scoring tools. Related systems such as ClinicalAgent [<xref ref-type="bibr" rid="B24-information-16-00894">24</xref>] and ColaCare [<xref ref-type="bibr" rid="B60-information-16-00894">60</xref>] extend this pipeline with personalized reasoning and cross-modality verification.</p>
          <p>Finally, robust multimodal integration contributes significantly to agent trustworthiness and robustness. Recent studies like Yi et al. [<xref ref-type="bibr" rid="B61-information-16-00894">61</xref>] and Almansoori et al. [<xref ref-type="bibr" rid="B62-information-16-00894">62</xref>] highlight the importance of modality alignment, hierarchical fusion, and adaptive control in building safe and interpretable biomedical agents.</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec4-information-16-00894">
      <title>4. Performance Evaluation and Benchmarking</title>
      <p>Having detailed core agent methodologies, we now turn to evaluating how biomedical LLM agents perform across realistic tasks and scenarios. As biomedical LLM agents continue to evolve, it is increasingly essential to develop rigorous and comprehensive evaluation methodologies that reflect their capabilities, reliability, and safety in real-world scenarios.</p>
      <sec id="sec4dot1-information-16-00894">
        <title>4.1. The Need for Agent-Specific Evaluation</title>
        <p>Traditional NLP benchmarks and static QA datasets such as MedQA (United States Medical Licensing Examination) are no longer adequate for evaluating LLM-based agents [<xref ref-type="bibr" rid="B63-information-16-00894">63</xref>]. These agents operate beyond passive language modeling; their core value lies in dynamic decision making processes, including sequential reasoning, adaptive planning in evolving environments [<xref ref-type="bibr" rid="B64-information-16-00894">64</xref>], external tool usage for calculations or database access [<xref ref-type="bibr" rid="B8-information-16-00894">8</xref>], multi-step inference in complex contexts [<xref ref-type="bibr" rid="B64-information-16-00894">64</xref>], and collaborative interactions with users or other agents [<xref ref-type="bibr" rid="B8-information-16-00894">8</xref>]. As a result, evaluation has shifted from assessing static knowledge recall to measuring task completion success, reasoning quality, and agent behavior under interactive conditions.</p>
      </sec>
      <sec id="sec4dot2-information-16-00894">
        <title>4.2. Key Benchmarks for Biomedical LLM Agents</title>
        <p>To meet these evaluation needs, several benchmarks have been proposed that simulate real-world biomedical applications across diverse dimensions of agent capability. These can be broadly grouped into benchmarks for interactive clinical simulation, tool-augmented reasoning, and trustworthiness or domain-specific assessment.</p>
        <p>Benchmarks for interactive clinical simulation include AgentClinic [<xref ref-type="bibr" rid="B8-information-16-00894">8</xref>], which evaluates agent performance in multimodal, role-driven clinical scenarios where agents act as doctors, patients, or moderators. It incorporates datasets such as AgentClinic-MedQA [<xref ref-type="bibr" rid="B8-information-16-00894">8</xref>], NEJM, and MIMIC-IV [<xref ref-type="bibr" rid="B65-information-16-00894">65</xref>], drawn from USMLE questions, clinical image challenges, and real-world EHRs. MedJourney [<xref ref-type="bibr" rid="B7-information-16-00894">7</xref>] is a Chinese-language benchmark designed to assess LLM performance throughout the entire patient journey from diagnosis to follow-up in 12 subtasks, including department recommendation, dialogue summarization, and medication Q&amp;A. It combines automated metrics (e.g., BLEU, recall) with human or LLM-based evaluations.</p>
        <p>For tool-augmented reasoning and execution, CalcQA [<xref ref-type="bibr" rid="B6-information-16-00894">6</xref>] evaluates agents&#x2019; ability to interpret clinical contexts, select appropriate calculators, convert units, and deliver final assessments. It comprises 44 calculators, 237 unit conversions, and 100 matched real-world cases. MedAgentBench [<xref ref-type="bibr" rid="B63-information-16-00894">63</xref>] benchmarks agent interaction with FHIR-compliant EHR systems through 300 structured tasks across 10 clinical categories. The generalist EHR agent developed by Song et al. [<xref ref-type="bibr" rid="B66-information-16-00894">66</xref>] similarly assesses clinical data manipulation across a 100-patient dataset.</p>
        <p>In the realm of trustworthiness and specialized domains, MedHal [<xref ref-type="bibr" rid="B67-information-16-00894">67</xref>] targets hallucination detection in clinical and QA contexts, while SourceCheckup [<xref ref-type="bibr" rid="B5-information-16-00894">5</xref>] evaluates the factual integrity of generated content by measuring citation validity and source alignment. BixBench [<xref ref-type="bibr" rid="B68-information-16-00894">68</xref>] focuses on bioinformatics applications, such as gene function prediction and molecular structure analysis. ArgMed-Agents [<xref ref-type="bibr" rid="B28-information-16-00894">28</xref>] tests explainable decision-making via structured argumentation frameworks for clinical reasoning.</p>
        <p>These benchmarks collectively assess biomedical LLM agents along different axes, including reasoning accuracy, tool use fidelity, and epistemic reliability. However, most are limited to static or task-specific evaluations and insufficiently capture dynamic agentic capabilities such as planning, memory, and tool orchestration. The development of holistic, standardized benchmark suites remains a critical priority. <xref ref-type="table" rid="information-16-00894-t002">Table 2</xref> provides an overview of major biomedical agent evaluation benchmarks.</p>
      </sec>
      <sec id="sec4dot3-information-16-00894">
        <title>4.3. Evaluation Indicators and Methods</title>
        <p>Evaluating biomedical LLM agents requires a multifaceted set of metrics and methodologies to capture their complex behaviors across different tasks and settings. Task success rate [<xref ref-type="bibr" rid="B63-information-16-00894">63</xref>] assesses whether the agent ultimately achieves its intended goal, such as issuing a correct diagnosis or generating an actionable experimental plan. Accuracy [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>] is commonly applied in classification and QA tasks to measure the proportion of correct outputs. Natural language generation (NLG) metrics such as BLEU, ROUGE, and BERTScore are used to quantify the similarity between generated and reference texts, particularly for summarization or open-ended generation tasks [<xref ref-type="bibr" rid="B7-information-16-00894">7</xref>].</p>
        <p>Qualitative metrics [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>], often based on human expert judgment (e.g., clinicians), employ Likert scales to evaluate dimensions such as clarity, completeness, usefulness, relevance, coherence, safety, and bias. Although subjective, such assessments provide clinical insight that automated metrics frequently overlook, but they also face challenges like inter-rater variability and high annotation costs. The LLM-as-judge approach [<xref ref-type="bibr" rid="B7-information-16-00894">7</xref>] offers a scalable alternative by leveraging powerful models such as GPT-4 to evaluate other agents&#x2019; outputs, though questions around its objectivity and reliability remain.</p>
        <p>In addition, agent-specific metrics [<xref ref-type="bibr" rid="B11-information-16-00894">11</xref>] are employed to evaluate planning quality, tool invocation accuracy, interaction efficiency (e.g., number of turns to task completion), and robustness to adversarial or biased inputs. Cost and efficiency indicators [<xref ref-type="bibr" rid="B39-information-16-00894">39</xref>] track operational resource usage, including API call frequency, latency, and computational overhead, which are critical for assessing practical feasibility.</p>
      </sec>
      <sec id="sec4dot4-information-16-00894">
        <title>4.4. Performance Comparison</title>
        <p>Recent evaluation studies offer several important insights into the performance dynamics of biomedical LLM agents. First, agent-based evaluation proves substantially more difficult than static QA. Models that perform well on datasets like MedQA often experience dramatic drops in accuracy, sometimes falling below one-tenth when the same questions are embedded in interactive diagnostic scenarios [<xref ref-type="bibr" rid="B63-information-16-00894">63</xref>,<xref ref-type="bibr" rid="B69-information-16-00894">69</xref>]. This highlights the inadequacy of static benchmarks in capturing agent capabilities within simulated clinical workflows.</p>
        <p>Second, significant performance variation exists across models. Proprietary LLMs such as GPT-4 and Claude 3.5 consistently outperform open-source counterparts in complex agent tasks [<xref ref-type="bibr" rid="B63-information-16-00894">63</xref>], particularly those requiring multi-step reasoning, tool usage, or sustained interaction. This underscores the current limitations of open models in agentic reasoning environments.</p>
        <p>Third, performance is notably enhanced through task-specific augmentation strategies. Retrieval-augmented generation (RAG) has been shown to significantly improve accuracy on clinical tasks [<xref ref-type="bibr" rid="B42-information-16-00894">42</xref>], while specialized tool use, such as OpenMedCal, improves computational precision and reduces error rates. Moreover, multi-agent frameworks, including MedAgents [<xref ref-type="bibr" rid="B21-information-16-00894">21</xref>], the MAC framework [<xref ref-type="bibr" rid="B22-information-16-00894">22</xref>], and GPT-4-based voting ensembles for medical diagnosis [<xref ref-type="bibr" rid="B70-information-16-00894">70</xref>,<xref ref-type="bibr" rid="B71-information-16-00894">71</xref>], consistently outperform single-agent setups in complex diagnostic reasoning. These findings emphasize the critical role of targeted enhancement and collaborative mechanisms in optimizing agent performance and clinical reliability.</p>
        <p>The development of these specialized, interactive benchmarks is an important trend in evaluating biomedical LLM agents [<xref ref-type="bibr" rid="B72-information-16-00894">72</xref>]. They shift the focus of evaluation from static knowledge recall to the actual utility and safety of agents in simulating real workflows. However, standardization of evaluation methods and standards remains a challenge that requires continued research and community consensus.</p>
      </sec>
    </sec>
    <sec id="sec5-information-16-00894">
      <title>5. Key Challenges and Mitigation Strategies</title>
      <p>Despite their promising potential, biomedical LLM agents face a spectrum of unresolved technical and ethical challenges.</p>
      <p>As illustrated in <xref ref-type="fig" rid="information-16-00894-f003">Figure 3</xref>, these challenges span multiple layers, from factual hallucinations and interpretability to tool integration, data bias, and regulatory governance. Each category is further mapped to mitigation strategies proposed in the recent literature, highlighting the need for a principled risk-aware design.</p>
      <sec id="sec5dot1-information-16-00894">
        <title>5.1. Hallucinations and Factual Inaccuracies</title>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>Challenge: LLM agents are prone to generating content that is plausible, but factually incorrect or unsupported, a phenomenon known as hallucination. In high-stakes biomedical applications, such errors can lead to harmful outcomes, including misdiagnoses, inappropriate treatment recommendations, and misleading scientific claims, thereby jeopardizing patient safety and research integrity. Manifestations include erroneous clinical calculations [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>], citing irrelevant or unsupported references [<xref ref-type="bibr" rid="B5-information-16-00894">5</xref>], fabricating nonexistent facts, or producing outputs that contradict input information [<xref ref-type="bibr" rid="B73-information-16-00894">73</xref>].</p>
          </list-item>
          <list-item>
            <label>(2)</label>
            <p>Mitigation Strategies: RAG [<xref ref-type="bibr" rid="B42-information-16-00894">42</xref>] mitigates hallucination by grounding model outputs in verified knowledge sources such as PubMed or clinical guidelines, though its reliability still requires careful validation [<xref ref-type="bibr" rid="B74-information-16-00894">74</xref>]. Delegating computation to external tools also helps reduce errors in quantitative reasoning tasks [<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>]. Self-correction mechanisms and reflection loops allow agents to evaluate and revise their outputs [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>], while systems like SourceCheckup [<xref ref-type="bibr" rid="B5-information-16-00894">5</xref>] support automated validation and transparent citation. Additionally, hallucination detection tools and purpose-built benchmarks (e.g., MedHal [<xref ref-type="bibr" rid="B67-information-16-00894">67</xref>]) assist in identifying factual inconsistencies [<xref ref-type="bibr" rid="B75-information-16-00894">75</xref>]. Prompt engineering [<xref ref-type="bibr" rid="B11-information-16-00894">11</xref>] also plays a role in reducing hallucinations by guiding the model toward evidence-based responses.</p>
          </list-item>
        </list>
      </sec>
      <sec id="sec5dot2-information-16-00894">
        <title>5.2. Explainability and Transparency</title>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>Challenge: The opaque nature of deep learning models makes it difficult to interpret the internal decision-making processes of LLMs [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>]. In biomedical settings, lack of interpretability can hinder clinical trust, adoption, and accountability [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>].</p>
          </list-item>
          <list-item>
            <label>(2)</label>
            <p>Mitigation Strategies: Various interpretability techniques, such as SHAP, LIME, and attention visualization, offer insights into model predictions [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>]. Reasoning transparency can be improved through structured prompting strategies like Chain-of-Thought (CoT), Chain-of-Diagnosis (CoD) [<xref ref-type="bibr" rid="B76-information-16-00894">76</xref>], or argumentation frameworks used in ArgMed- Agents [<xref ref-type="bibr" rid="B28-information-16-00894">28</xref>]. Integrating structured knowledge graphs grounds reasoning in transparent semantic relations [<xref ref-type="bibr" rid="B2-information-16-00894">2</xref>], while model cards provide standardized documentation on training data, performance, and limitations [<xref ref-type="bibr" rid="B77-information-16-00894">77</xref>]. Human-in-the-loop designs also enhance transparency by allowing users to query and critique model outputs interactively [<xref ref-type="bibr" rid="B78-information-16-00894">78</xref>].</p>
          </list-item>
        </list>
      </sec>
      <sec id="sec5dot3-information-16-00894">
        <title>5.3. Data Quality, Availability, and Bias</title>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>Challenge: LLM agent performance is highly sensitive to data quality and diversity [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>]. Biomedical datasets are often limited in size, poorly annotated, or hypothetical in nature [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>]. More critically, training data may encode biases related to race, gender, or socioeconomic status, which can lead to inequitable care recommendations [<xref ref-type="bibr" rid="B12-information-16-00894">12</xref>,<xref ref-type="bibr" rid="B79-information-16-00894">79</xref>]. For instance, models may misdiagnose skin conditions in patients with darker skin tones or suggest higher-cost treatments based on demographic profiles. These biases may be further amplified by feedback loops and dataset reuse [<xref ref-type="bibr" rid="B79-information-16-00894">79</xref>].</p>
          </list-item>
          <list-item>
            <label>(2)</label>
            <p>Mitigation Strategies: High-quality, diverse, and rigorously curated datasets are foundational to reducing bias [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>]. Bias mitigation should be implemented across the entire pipeline, from data collection to training and evaluation, using techniques such as adversarial debiasing, data augmentation, and fairness-aware optimization [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>]. Ongoing auditing of model outputs across demographic groups is critical [<xref ref-type="bibr" rid="B77-information-16-00894">77</xref>], as is stakeholder involvement during dataset development to ensure inclusive representation and equity [<xref ref-type="bibr" rid="B77-information-16-00894">77</xref>].</p>
          </list-item>
        </list>
      </sec>
      <sec id="sec5dot4-information-16-00894">
        <title>5.4. Tool Reliability and Integration</title>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>Challenge: LLM agents rely heavily on external tools for computation, retrieval, and decision support [<xref ref-type="bibr" rid="B20-information-16-00894">20</xref>]. Failures in tool reliability, outdated resources, or unstable interfaces can compromise agent performance. Additionally, agents often struggle with understanding tool functions, parameter usage, and invocation contexts [<xref ref-type="bibr" rid="B34-information-16-00894">34</xref>], leading to misuse or overdependence [<xref ref-type="bibr" rid="B34-information-16-00894">34</xref>].</p>
          </list-item>
          <list-item>
            <label>(2)</label>
            <p>Mitigation Strategies: Ensuring tool robustness requires continuous development, validation, and API standardization [<xref ref-type="bibr" rid="B20-information-16-00894">20</xref>]. Enhancing agent comprehension of tool documentation through fine-tuning or prompt optimization improves tool selection and usage accuracy [<xref ref-type="bibr" rid="B6-information-16-00894">6</xref>]. Benchmarks such as Medcalc-Bench [<xref ref-type="bibr" rid="B45-information-16-00894">45</xref>] provide structured assessments of agents&#x2019; ability to execute clinical computations via APIs, including drug dosing, renal function estimation, and risk scoring. Finally, incorporating error-handling mechanisms enables agents to respond gracefully to tool failures, improving overall reliability.</p>
          </list-item>
        </list>
      </sec>
      <sec id="sec5dot5-information-16-00894">
        <title>5.5. Multimodal Data Integration and Processing</title>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>Challenge: Biomedical agents must often integrate heterogeneous data modalities text, images, genomics, and EHRs, which differ in format, scale, and sparsity [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>,<xref ref-type="bibr" rid="B80-information-16-00894">80</xref>]. Aligning and jointly analyzing these modalities in a unified framework remains technically challenging, especially under incomplete or noisy conditions [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>].</p>
          </list-item>
          <list-item>
            <label>(2)</label>
            <p>Mitigation Strategies: Progress requires more advanced fusion architectures (e.g., gated and tensor fusion, Transformer-based joint encoders) capable of modeling cross-modal relationships [<xref ref-type="bibr" rid="B81-information-16-00894">81</xref>]. Robust alignment techniques are essential to link data at the semantic level [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>]. Agents must also handle missing data gracefully, which calls for models explicitly designed for partial or sparse input [<xref ref-type="bibr" rid="B81-information-16-00894">81</xref>]. Building comprehensive multimodal datasets and evaluation benchmarks is equally crucial for meaningful progress [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>].</p>
          </list-item>
        </list>
      </sec>
      <sec id="sec5dot6-information-16-00894">
        <title>5.6. Complexity of Multi-Agent Collaboration</title>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>Challenge: Multi-agent systems (MAS) offer enhanced flexibility and specialization but introduce challenges such as task allocation, inter-agent reasoning coordination, and communication overhead [<xref ref-type="bibr" rid="B35-information-16-00894">35</xref>,<xref ref-type="bibr" rid="B82-information-16-00894">82</xref>,<xref ref-type="bibr" rid="B83-information-16-00894">83</xref>]. In biomedical workflows, failures in coordination can lead to suboptimal or erroneous outcomes [<xref ref-type="bibr" rid="B39-information-16-00894">39</xref>].</p>
          </list-item>
          <list-item>
            <label>(2)</label>
            <p>Mitigation Strategies: Optimizing collaboration requires adaptive control schemes (MDAgents [<xref ref-type="bibr" rid="B39-information-16-00894">39</xref>]), hierarchical role assignment (KG4Diagnosis [<xref ref-type="bibr" rid="B23-information-16-00894">23</xref>]), and efficient communication protocols [<xref ref-type="bibr" rid="B41-information-16-00894">41</xref>]. Consensus mechanisms, such as supervisory agents, can arbitrate disagreements and ensure reliability. Explicit role specialization also reduces redundancy and conflict. For instance, MedCo demonstrates how agents can function as educators, students, and evaluators in a simulated medical training context [<xref ref-type="bibr" rid="B84-information-16-00894">84</xref>].</p>
          </list-item>
        </list>
      </sec>
      <sec id="sec5dot7-information-16-00894">
        <title>5.7. Ethics, Privacy, Security, and Regulation</title>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>Challenge: Ethical, legal, and social concerns are among the most pressing challenges in deploying biomedical LLM agents. Risks include algorithmic bias that may exacerbate healthcare inequities [<xref ref-type="bibr" rid="B12-information-16-00894">12</xref>], erosion of traditional patient&#x2013;clinician relationships [<xref ref-type="bibr" rid="B85-information-16-00894">85</xref>], unclear accountability in cases of AI-induced medical error [<xref ref-type="bibr" rid="B12-information-16-00894">12</xref>], overreliance on AI systems [<xref ref-type="bibr" rid="B75-information-16-00894">75</xref>], and disputes around intellectual property of AI-generated content [<xref ref-type="bibr" rid="B35-information-16-00894">35</xref>].</p>
          </list-item>
        </list>
        <p>Privacy and security concerns arise from handling large volumes of sensitive patient data, with threats such as data leakage, re-identification, or misuse [<xref ref-type="bibr" rid="B12-information-16-00894">12</xref>]. Agent interactions with external tools or other agents may also introduce new attack surfaces and cybersecurity vulnerabilities [<xref ref-type="bibr" rid="B86-information-16-00894">86</xref>]. Compliance with regulations like HIPAA and GDPR is essential [<xref ref-type="bibr" rid="B87-information-16-00894">87</xref>], but current regulatory frameworks (e.g., FDA, EU AI Act) may not fully account for the dynamic, emergent behavior of LLM agents [<xref ref-type="bibr" rid="B63-information-16-00894">63</xref>]. The absence of clear approval pathways, accountability standards, and post-deployment oversight further compounds regulatory uncertainty [<xref ref-type="bibr" rid="B63-information-16-00894">63</xref>].</p>
        <list list-type="order">
          <list-item>
            <label>(2)</label>
            <p>Mitigation Strategies: To address these concerns, ethical principles such as fairness, transparency, accountability, and safety must be embedded throughout the agent lifecycle [<xref ref-type="bibr" rid="B88-information-16-00894">88</xref>]. Privacy-preserving techniques, including data anonymization, differential privacy, federated learning, secure multi-party computation, and synthetic data generation, help protect sensitive information [<xref ref-type="bibr" rid="B35-information-16-00894">35</xref>]. Strengthening cybersecurity through adversarial testing and secure interface design is also essential [<xref ref-type="bibr" rid="B35-information-16-00894">35</xref>].</p>
          </list-item>
        </list>
        <p>In parallel, comprehensive governance frameworks and AI-specific regulatory standards are needed to clarify stakeholder responsibilities and ensure safe deployment [<xref ref-type="bibr" rid="B89-information-16-00894">89</xref>]. Increasing algorithmic transparency and instituting robust accountability mechanisms are critical steps toward trustworthy and responsible AI use in biomedical contexts [<xref ref-type="bibr" rid="B12-information-16-00894">12</xref>].</p>
        <p>The matrix in <xref ref-type="table" rid="information-16-00894-t003">Table 3</xref> provides a principled lens to understand emerging risks, map mitigation pathways, and design appropriate assessment instruments. Trustworthy AI in biomedicine is not merely a technical objective, but a fundamental requirement for real-world adoption. Addressing these challenges requires sustained collaboration across technical, ethical, legal, and clinical domains.</p>
      </sec>
    </sec>
    <sec sec-type="discussion" id="sec6-information-16-00894">
      <title>6. Discussion</title>
      <sec id="sec6dot1-information-16-00894">
        <title>6.1. Major Insights from the Current Landscape</title>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>The necessity of Human-in-the-Loop: Despite rapid advancements in autonomous LLM agents, the integration of Human-in-the-Loop (HITL) mechanisms remains indispensable for biomedical applications. Given the high-stakes nature of clinical decision-making, full agent autonomy in tasks such as diagnosis, treatment planning, or data interpretation is neither ethically acceptable nor technically safe. HITL frameworks enable real-time expert supervision, enhance system transparency, and act as safeguards against hallucinations, biases, and misuse of external tools. Prior studies, including the Zodiac framework [<xref ref-type="bibr" rid="B90-information-16-00894">90</xref>] and virtual lab agents [<xref ref-type="bibr" rid="B91-information-16-00894">91</xref>], underscore how clinician&#x2013;agent collaboration can not only mitigate risk but also improve performance through iterative feedback and correction. Recent developments have also explored internal supervision via multi-agent architectures. For example, Cui et al. [<xref ref-type="bibr" rid="B92-information-16-00894">92</xref>] proposed a dual-agent model in which a critical agent dynamically monitors and adjusts the reasoning process of a predictive agent&#x2014;effectively emulating internalized HITL oversight. These findings collectively support the view that biomedical agents should be explicitly designed with human supervision interfaces, particularly in regulation-sensitive domains such as oncology, radiology, and pharmacovigilance.</p>
          </list-item>
          <list-item>
            <label>(2)</label>
            <p>Biomedical full-spectrum applications: from basic research to clinical implementation: LLM agents are rapidly emerging as generalizable tools capable of driving innovation across the entire biomedical pipeline. From early-stage hypothesis generation and drug discovery to downstream tasks like clinical trial matching, diagnostic reasoning, and personalized care planning, these systems are demonstrating substantial potential to accelerate, scale, and automate workflows traditionally limited by human capacity. Notable examples such as MedAgent-Pro [<xref ref-type="bibr" rid="B93-information-16-00894">93</xref>] and FUAS-Agents [<xref ref-type="bibr" rid="B94-information-16-00894">94</xref>] illustrate how agents can operate within structured, rule-based clinical environments like surgical decision-making or protocol-driven diagnosis. <xref ref-type="fig" rid="information-16-00894-f004">Figure 4</xref> summarizes representative agent workflows across biomedical domains, including healthcare optimization, dialogue simulation, scientific research, and clinical decision support. Simultaneously, open-ended agents like the &#x201C;virtual laboratory&#x201D; platform show promise for creativity and hypothesis exploration in basic science contexts, such as antibody design. Collectively, these developments support the growing consensus that biomedical LLM agents are not confined to isolated use cases but instead represent a versatile computational paradigm capable of supporting diverse biomedical tasks.</p>
          </list-item>
        </list>
      </sec>
      <sec id="sec6dot2-information-16-00894">
        <title>6.2. Proposed New Metrics for Agent Robustness and Trustworthiness</title>
        <p>While existing benchmarks evaluate agent accuracy or task completion, they often overlook dimensions such as factual grounding, transparency, and resilience. Purpose-built metrics like hallucination rates [<xref ref-type="bibr" rid="B67-information-16-00894">67</xref>], demographic bias scores [<xref ref-type="bibr" rid="B79-information-16-00894">79</xref>], citation integrity [<xref ref-type="bibr" rid="B5-information-16-00894">5</xref>], and reasoning traceability [<xref ref-type="bibr" rid="B28-information-16-00894">28</xref>] are essential complements to traditional evaluation. Multidimensional benchmarking suites that capture error detection, self-reflection, and multi-agent coordination are urgently needed to assess trustworthiness holistically.</p>
      </sec>
      <sec id="sec6dot3-information-16-00894">
        <title>6.3. Toward Standardized Safety Evaluation</title>
        <p>Mitigating biomedical agent risks is not a one-time engineering fix but an ongoing process of validation, governance, and user alignment. Standardized evaluation frameworks must address both task-specific performance and broader dimensions of safety, privacy, and fairness. Integration of emerging techniques like RAG, reflective prompting, federated learning, and secure multi-agent systems will be critical. Ultimately, trustworthy biomedical AI demands cross-disciplinary collaboration across technical, regulatory, and clinical communities.</p>
      </sec>
      <sec id="sec6dot4-information-16-00894">
        <title>6.4. Strategic Perspectives for Future Development</title>
        <p>The field of biomedical LLM agents is evolving rapidly, and several strategic directions are emerging to support their safe, effective, and scalable integration into real-world applications.</p>
        <list list-type="order">
          <list-item>
            <label>(1)</label>
            <p>Enhanced Medical Reasoning and Planning Capabilities: Despite substantial progress, current biomedical LLM agents still face limitations in handling complex, long-range reasoning and multi-step task planning [<xref ref-type="bibr" rid="B28-information-16-00894">28</xref>]. Future research is expected to explore multiple complementary directions.</p>
            <p>First, advances in foundation models such as DeepSeek R1 [<xref ref-type="bibr" rid="B95-information-16-00894">95</xref>] offer improved capabilities in long-context comprehension, instruction following, and nuanced medical reasoning. Second, new reasoning paradigms are being introduced beyond conventional Chain-of-Thought (CoT) and ReAct strategies. These include symbolic logic frameworks [<xref ref-type="bibr" rid="B96-information-16-00894">96</xref>], causal inference-based models [<xref ref-type="bibr" rid="B97-information-16-00894">97</xref>], and hierarchical planning algorithms that can better support high-stakes clinical decision-making [<xref ref-type="bibr" rid="B28-information-16-00894">28</xref>]. Third, the ability of agents to self-assess their confidence and recognize knowledge gaps is gaining attention as a mechanism to ensure safety and reliability [<xref ref-type="bibr" rid="B98-information-16-00894">98</xref>]. Additionally, adaptive architectures that dynamically restructure based on task requirements may offer further gains in generalization and robustness [<xref ref-type="bibr" rid="B99-information-16-00894">99</xref>].</p>
          </list-item>
          <list-item>
            <label>(2)</label>
            <p>Continual Learning and Knowledge Updating: Given the fast pace of biomedical research, agents must maintain mechanisms for continuous learning and timely knowledge integration to prevent the use of outdated or invalid information [<xref ref-type="bibr" rid="B12-information-16-00894">12</xref>].</p>
            <p>This need motivates future efforts in three key directions. First, efficient incorporation of new clinical guidelines, medical literature, and experimental findings, potentially through hybrid strategies combining RAG and fine-tuning, will be essential. Second, lifelong or continual learning frameworks [<xref ref-type="bibr" rid="B100-information-16-00894">100</xref>] must be designed to allow agents to update incrementally while preserving previously acquired knowledge [<xref ref-type="bibr" rid="B101-information-16-00894">101</xref>]. Third, while federated learning offers strong privacy protection, its deployment in real medical environments faces challenges such as data heterogeneity, communication latency, and privacy&#x2013;utility trade-offs. Recent studies have proposed validated mitigation strategies, including adaptive optimization algorithms (e.g., FedProx, FedDyn) to address data imbalance, secure aggregation and homomorphic encryption to prevent information leakage, and federated differential privacy mechanisms for enhanced protection of gradient updates. Preliminary biomedical applications, such as the FLUID framework [<xref ref-type="bibr" rid="B102-information-16-00894">102</xref>] and MedFL [<xref ref-type="bibr" rid="B103-information-16-00894">103</xref>], have demonstrated that incorporating these strategies enables robust distributed learning and continual adaptation across hospitals without exposing sensitive patient data. These approaches represent a significant step toward trustworthy and collaborative AI in healthcare.</p>
          </list-item>
          <list-item>
            <label>(3)</label>
            <p>Standardization, Governance, and Regulation: To enable trustworthy and equitable deployment, future work must establish clear standards for benchmarking, validation, and risk management [<xref ref-type="bibr" rid="B104-information-16-00894">104</xref>]. Standardization efforts should aim to define consistent evaluation metrics and community benchmarks [<xref ref-type="bibr" rid="B105-information-16-00894">105</xref>]. At the governance level, key priorities include ensuring algorithmic transparency, responsible data use, and stakeholder accountability throughout the agent lifecycle [<xref ref-type="bibr" rid="B106-information-16-00894">106</xref>]. Regulatory frameworks must evolve to address the dynamic nature of AI systems, adopting mechanisms such as continuous monitoring, adaptive certification, and risk-sensitive regulation tailored to specific use cases and deployment environments [<xref ref-type="bibr" rid="B89-information-16-00894">89</xref>].</p>
          </list-item>
          <list-item>
            <label>(4)</label>
            <p>Expanding and Deepening Multimodal Capabilities: As biomedical data becomes increasingly diverse and multimodal, agents must be equipped to interpret and reason over heterogeneous inputs, including clinical notes, imaging, genomics, physiological signals, and patient-generated data [<xref ref-type="bibr" rid="B51-information-16-00894">51</xref>]. Future progress depends on the development of advanced multimodal fusion architectures that can model intermodal dependencies and contextual relationships more effectively. There is also a pressing need to integrate emerging modalities such as wearable sensor data, proteomics, and behavioral analytics into unified reasoning frameworks. Importantly, improving the interpretability and traceability of multimodal reasoning processes will be key to enabling clinical trust and accountability.</p>
          </list-item>
          <list-item>
            <label>(5)</label>
            <p>Human&#x2013;AI Collaboration and Interaction: Rather than functioning as replacements for human professionals, future biomedical LLM agents should serve as collaborative partners that augment human expertise [<xref ref-type="bibr" rid="B13-information-16-00894">13</xref>]. This calls for the design of more natural and efficient interaction interfaces, enabling users to guide, correct, and query agent behavior in real time. Collaborative decision-making protocols must be developed to support shared agency and role clarity between humans and machines. Furthermore, cultivating user trust will require increased transparency, reliability, and user control over agent outputs especially in high-risk or legally sensitive domains.</p>
          </list-item>
          <list-item>
            <label>(6)</label>
            <p>Toward Trustworthy AI: Ultimately, the success of biomedical LLM agents will hinge on their alignment with both technical standards and broader societal values [<xref ref-type="bibr" rid="B107-information-16-00894">107</xref>]. Building truly trustworthy systems requires integration across all preceding fronts from robust model architectures and continual learning protocols to ethical design, regulatory compliance, and meaningful stakeholder engagement. These agents must not only function reliably in technical terms but also earn the confidence of clinicians, patients, and regulatory bodies.</p>
          </list-item>
        </list>
        <p>To achieve this, agent development should incorporate fairness, accountability, transparency, and safety as core design principles from the outset. Human oversight must be embedded both during the training and deployment phases, enabling clinicians to monitor, guide, and correct agent behavior. Governance frameworks should ensure algorithmic transparency, clear auditability of decisions, and safeguards against misuse or systemic bias.</p>
        <p>In parallel, evaluation paradigms must evolve beyond static and task-isolated benchmarks to reflect the dynamic, interactive, and high-stakes nature of real-world biomedical applications. Trustworthiness should be assessed not only by accuracy but also through multifaceted criteria such as hallucination detection, citation fidelity, bias quantification, robustness under uncertainty, and clinical safety. Longitudinal, scenario-based, and simulation-driven benchmarks grounded in realistic agent workflows will be crucial to capturing these aspects comprehensively.</p>
        <p>Standardizing these evaluation criteria, along with establishing open, reproducible, and community-endorsed benchmarking frameworks, is essential to enable transparent model comparison, support regulatory validation, and foster public trust. In this light, advancing evaluation methodology is not merely a technical task but a foundational requirement for building socially acceptable and ethically aligned biomedical LLM agents. The path forward will require sustained interdisciplinary collaboration among technologists, clinicians, ethicists, and policymakers to ensure that future agents are not only effective but also accountable, equitable, and human-centered.</p>
      </sec>
      <sec id="sec6dot5-information-16-00894">
        <title>6.5. Future Research Roadmap and Strategic Priorities</title>
        <p>Beyond these specific research priorities, the next five years should also focus on establishing an integrative scientific foundation that unifies methodological innovation with translational impact. Our analysis reveals that true progress in biomedical LLM agents will depend on developing meta-level intelligence systems capable not only of performing reasoning and tool orchestration, but also of understanding when and how to apply them. This requires embedding adaptive self-regulation, uncertainty quantification, and epistemic awareness into agent architectures, enabling models to recognize the limits of their competence and seek human or algorithmic validation accordingly. Moreover, the convergence of federated learning, multimodal integration, and human AI collaboration offers an unprecedented opportunity to construct continuously learning ecosystems, where insights derived from distributed institutions can be aggregated securely to refine collective biomedical knowledge. Achieving this vision will demand not just technical sophistication but a co-evolution of governance, evaluation, and design ethics where trustworthiness becomes an operational property of the system rather than a post hoc assessment. Ultimately, advancing toward this paradigm will redefine biomedical LLM agents from passive assistants to self-reflective scientific collaborators that augment discovery, decision-making, and clinical reasoning in an accountable and interpretable manner.</p>
      </sec>
    </sec>
    <sec sec-type="conclusions" id="sec7-information-16-00894">
      <title>7. Conclusions</title>
      <p>Biomedical LLM agent systems that integrate language models with capabilities such as planning, memory, tool use, and interaction are transforming biomedical AI from passive information processing to active, goal-driven reasoning and decision-making. Recent advances span both single-agent architectures, which enhance internal mechanisms like self-reflection and long-range planning, and multi-agent frameworks that simulate collaborative clinical workflows. Techniques such as RAG, domain-adaptive fine-tuning, tool orchestration, and multimodal integration are increasingly applied in combination, forming hybrid, specialized agents tailored to biomedical tasks. Yet significant challenges remain: current evaluation benchmarks insufficiently capture the complexity and dynamism of real-world applications; hallucinations, limited interpretability, and data bias raise concerns about safety, equity, and trust; and privacy protection, tool reliability, and regulatory alignment pose further deployment barriers. To address these gaps, future efforts must focus on enhancing reasoning capabilities, enabling continual knowledge updating, standardizing evaluation protocols, deepening multimodal processing, and developing more intuitive human&#x2013;AI interfaces. Ultimately, the goal is to build trustworthy biomedical agents that are not only technically robust but also clinically meaningful, ethically aligned, and practically deployable. Achieving this vision will require sustained innovation and close collaboration across disciplines, bringing together AI researchers, clinicians, ethicists, and policymakers to co-design intelligent systems that can safely and effectively advance biomedical research and healthcare delivery.</p>
    </sec>
  </body>
  <back>
    <notes>
      <title>Institutional Review Board Statement</title>
      <p>Not applicable.</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      <p>Not applicable.</p>
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>No new data were created or analyzed in this study.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="B1-information-16-00894">
        <label>1.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>A comprehensive survey of scientific large language models and their applications in scientific discovery</article-title>
          <source>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</source>
          <conf-loc>Miami, FL, USA</conf-loc>
          <conf-date>12&#x2013;16 November 2024</conf-date>
          <fpage>8783</fpage>
          <lpage>8817</lpage>
        </element-citation>
      </ref>
      <ref id="B2-information-16-00894">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Gu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>Z.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A survey on medical large language models: Technology, application, trustworthiness, and future directions</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2406.03712</pub-id>
          <pub-id pub-id-type="arxiv">2406.03712</pub-id>
        </element-citation>
      </ref>
      <ref id="B3-information-16-00894">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gao</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Giunchiglia</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Noori</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Schwarz</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Ektefaie</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Kondic</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zitnik</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Empowering biomedical discovery with ai agents</article-title>
          <source>Cell</source>
          <year>2024</year>
          <volume>187</volume>
          <fpage>6125</fpage>
          <lpage>6151</lpage>
          <pub-id pub-id-type="doi">10.1016/j.cell.2024.09.022</pub-id>
        </element-citation>
      </ref>
      <ref id="B4-information-16-00894">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Goodell</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>S.N.</given-names>
            </name>
            <name>
              <surname>Rouholiman</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>L.F.</given-names>
            </name>
          </person-group>
          <article-title>Large language model agents can use tools to perform clinical calculations</article-title>
          <source>npj Digit. Med.</source>
          <year>2025</year>
          <volume>8</volume>
          <fpage>163</fpage>
          <pub-id pub-id-type="doi">10.1038/s41746-025-01475-8</pub-id>
        </element-citation>
      </ref>
      <ref id="B5-information-16-00894">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Casasola</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Nguyen</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Riantawan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ho</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>An automated framework for assessing how well llms cite relevant medical references</article-title>
          <source>Nat. Commun.</source>
          <year>2025</year>
          <volume>16</volume>
          <fpage>3615</fpage>
          <pub-id pub-id-type="doi">10.1038/s41467-025-58551-6</pub-id>
        </element-citation>
      </ref>
      <ref id="B6-information-16-00894">
        <label>6.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Xue</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>MeNTi: Bridging medical calculator and LLM agent with nested tool calling</article-title>
          <source>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</source>
          <conf-loc>Albuquerque, NM, USA</conf-loc>
          <conf-date>29 April&#x2013;4 May 2025</conf-date>
          <fpage>5097</fpage>
          <lpage>5116</lpage>
        </element-citation>
      </ref>
      <ref id="B7-information-16-00894">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ouyang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Medjourney: Benchmark and evaluation of large language models over patient clinical journey</article-title>
          <source>Adv. Neural Inf. Process. Syst.</source>
          <year>2024</year>
          <volume>37</volume>
          <fpage>87621</fpage>
          <lpage>87646</lpage>
        </element-citation>
      </ref>
      <ref id="B8-information-16-00894">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schmidgall</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ziaei</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Harris</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Reis</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Jopling</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Moor</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Agentclinic: A multimodal agent benchmark to evaluate ai in simulated clinical environments</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2405.07960</pub-id>
          <pub-id pub-id-type="arxiv">2405.07960</pub-id>
        </element-citation>
      </ref>
      <ref id="B9-information-16-00894">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Qu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Roohani</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Di</surname>
              <given-names>Y.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Biomni: A general-purpose biomedical ai agent</article-title>
          <source>bioRxiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.1101/2025.05.30.656746</pub-id>
          <pub-id pub-id-type="pmid">40501924</pub-id>
        </element-citation>
      </ref>
      <ref id="B10-information-16-00894">
        <label>10.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Fan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xue</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ruan</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>An llm-based framework for biomedical terminology normalization in social media via multi-agent collaboration</article-title>
          <source>Proceedings of the 31st International Conference on Computational Linguistics</source>
          <conf-loc>Abu Dhabi, United Arab Emirates</conf-loc>
          <conf-date>19&#x2013;24 January 2025</conf-date>
          <fpage>10712</fpage>
          <lpage>10726</lpage>
        </element-citation>
      </ref>
      <ref id="B11-information-16-00894">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Luo</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhuang</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>From intention to implementation: Automating biomedical research via LLMs</article-title>
          <source>Sci. China Inf. Sci.</source>
          <year>2025</year>
          <volume>68</volume>
          <fpage>170105</fpage>
          <pub-id pub-id-type="doi">10.1007/s11432-024-4485-0</pub-id>
        </element-citation>
      </ref>
      <ref id="B12-information-16-00894">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Qin</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Opportunities and challenges for large language models in primary health care</article-title>
          <source>J. Prim. Care Community Health</source>
          <year>2025</year>
          <volume>16</volume>
          <fpage>21501319241312571</fpage>
        </element-citation>
      </ref>
      <ref id="B13-information-16-00894">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>A survey of llm-based agents in medicine: How far are we from baymax?</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2502.11211</pub-id>
          <pub-id pub-id-type="arxiv">2502.11211</pub-id>
        </element-citation>
      </ref>
      <ref id="B14-information-16-00894">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yoon</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>So</surname>
              <given-names>C.H.</given-names>
            </name>
            <name>
              <surname>Kang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Biobert: A pre-trained biomedical language representation model for biomedical text mining</article-title>
          <source>Bioinformatics</source>
          <year>2020</year>
          <volume>36</volume>
          <fpage>1234</fpage>
          <lpage>1240</lpage>
          <pub-id pub-id-type="doi">10.1093/bioinformatics/btz682</pub-id>
        </element-citation>
      </ref>
      <ref id="B15-information-16-00894">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Tinn</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lucas</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Usuyama</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Naumann</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Poon</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Domain-specific language model pretraining for biomedical natural language processing</article-title>
          <source>ACM Trans. Comput. Healthc. (HEALTH)</source>
          <year>2021</year>
          <volume>3</volume>
          <fpage>1</fpage>
          <lpage>23</lpage>
          <pub-id pub-id-type="doi">10.1145/3458754</pub-id>
        </element-citation>
      </ref>
      <ref id="B16-information-16-00894">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Swanson</surname>
              <given-names>D.R.</given-names>
            </name>
          </person-group>
          <article-title>Fish oil, raynaud&#x2019;s syndrome, and undiscovered public knowledge</article-title>
          <source>Perspect. Biol. Med.</source>
          <year>1986</year>
          <volume>30</volume>
          <fpage>7</fpage>
          <lpage>18</lpage>
          <pub-id pub-id-type="doi">10.1353/pbm.1986.0087</pub-id>
        </element-citation>
      </ref>
      <ref id="B17-information-16-00894">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Swanson</surname>
              <given-names>D.R.</given-names>
            </name>
          </person-group>
          <article-title>Migraine and magnesium: Eleven neglected connections</article-title>
          <source>Perspect. Biol. Med.</source>
          <year>1988</year>
          <volume>31</volume>
          <fpage>526</fpage>
          <lpage>557</lpage>
          <pub-id pub-id-type="doi">10.1353/pbm.1988.0009</pub-id>
        </element-citation>
      </ref>
      <ref id="B18-information-16-00894">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gilardi</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Alizadeh</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kubli</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Chatgpt outperforms crowd workers for text-annotation tasks</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2023</year>
          <volume>120</volume>
          <fpage>e2305016120</fpage>
          <pub-id pub-id-type="doi">10.1073/pnas.2305016120</pub-id>
        </element-citation>
      </ref>
      <ref id="B19-information-16-00894">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Abbasian</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Azimi</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Rahmani</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Jain</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Conversational health agents: A personalized llm-powered agent framework</article-title>
          <source>arXiv</source>
          <year>2023</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2310.02374</pub-id>
          <pub-id pub-id-type="arxiv">2310.02374</pub-id>
        </element-citation>
      </ref>
      <ref id="B20-information-16-00894">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ramos</surname>
              <given-names>M.C.</given-names>
            </name>
            <name>
              <surname>Collison</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>White</surname>
              <given-names>A.D.</given-names>
            </name>
          </person-group>
          <article-title>A review of large language models and autonomous agents in chemistry</article-title>
          <source>Chem. Sci.</source>
          <year>2025</year>
          <volume>16</volume>
          <fpage>2514</fpage>
          <lpage>2572</lpage>
          <pub-id pub-id-type="doi">10.1039/D4SC03921A</pub-id>
        </element-citation>
      </ref>
      <ref id="B21-information-16-00894">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Cohan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gerstein</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>MedAgents: Large language models as collaborators for zero-shot medical reasoning</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2311.10537</pub-id>
        </element-citation>
      </ref>
      <ref id="B22-information-16-00894">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>You</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>G.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Enhancing diagnostic capability with multi-agents conversational large language models</article-title>
          <source>NPJ Digit. Med.</source>
          <year>2025</year>
          <volume>8</volume>
          <fpage>159</fpage>
        </element-citation>
      </ref>
      <ref id="B23-information-16-00894">
        <label>23.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zuo</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Mo</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Lio</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Kg4diagnosis: A hierarchical multi-agent llm framework with knowledge graph enhancement for medical diagnosis</article-title>
          <source>Proceedings of the AAAI Bridge Program on AI for Medicine and Healthcare. PMLR</source>
          <conf-loc>Philadelphia, PA, USA</conf-loc>
          <conf-date>25 February 2025</conf-date>
          <fpage>195</fpage>
          <lpage>204</lpage>
        </element-citation>
      </ref>
      <ref id="B24-information-16-00894">
        <label>24.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Yue</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Clinicalagent: Clinical trial multi-agent system with large language model-based reasoning</article-title>
          <source>Proceedings of the 15th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</source>
          <conf-loc>Shenzhen, China</conf-loc>
          <conf-date>22&#x2013;25 November 2024</conf-date>
          <fpage>1</fpage>
          <lpage>10</lpage>
        </element-citation>
      </ref>
      <ref id="B25-information-16-00894">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Qu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cousins</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>W.A.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Altman</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Cong</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Crispr-gpt: An llm agent for automated design of gene-editing experiments</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2404.18021</pub-id>
        </element-citation>
      </ref>
      <ref id="B26-information-16-00894">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Roohani</surname>
              <given-names>Y.H.</given-names>
            </name>
            <name>
              <surname>Vora</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Leskovec</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>BioDiscoveryAgent: An AI agent for designing genetic perturbation experiments</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2405.17631</pub-id>
        </element-citation>
      </ref>
      <ref id="B27-information-16-00894">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Das</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Maheswari</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Siddiqui</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Arora</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Paul</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Nanshi</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ud-balkar</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Sarvade</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chaturvedi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Shvartsman</surname>
              <given-names>T.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Improved precision oncology question-answering using agentic llm</article-title>
          <source>medRxiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.1101/2024.09.20.24314076</pub-id>
        </element-citation>
      </ref>
      <ref id="B28-information-16-00894">
        <label>28.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Hong</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Argmed-agents: Explainable clinical decision reasoning with llm disscusion via argumentation schemes</article-title>
          <source>Proceedings of the 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source>
          <conf-loc>Lisboa, Portugal</conf-loc>
          <conf-date>3&#x2013;6 December 2024</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2024</year>
          <fpage>5486</fpage>
          <lpage>5493</lpage>
        </element-citation>
      </ref>
      <ref id="B29-information-16-00894">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chan</surname>
              <given-names>T.K.</given-names>
            </name>
            <name>
              <surname>Dinh</surname>
              <given-names>N.-D.</given-names>
            </name>
          </person-group>
          <article-title>Entagents: Ai agents for complex knowledge otolaryngology</article-title>
          <source>medRxiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.1101/2025.01.01.25319863</pub-id>
        </element-citation>
      </ref>
      <ref id="B30-information-16-00894">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Luo</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ning</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>Y.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Taiyi: A bilingual fine-tuned large language model for diverse biomedical tasks</article-title>
          <source>J. Am. Med. Inform. Assoc.</source>
          <year>2024</year>
          <volume>31</volume>
          <fpage>1865</fpage>
          <lpage>1874</lpage>
          <pub-id pub-id-type="doi">10.1093/jamia/ocae037</pub-id>
        </element-citation>
      </ref>
      <ref id="B31-information-16-00894">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Medbiolm: Optimizing medical and biological qa with fine-tuned large language models and retrieval-augmented generation</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2502.03004</pub-id>
        </element-citation>
      </ref>
      <ref id="B32-information-16-00894">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hsu</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Roberts</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Llm-ie: A python package for biomedical generative information extraction with large language models</article-title>
          <source>JAMIA Open</source>
          <year>2025</year>
          <volume>8</volume>
          <fpage>ooaf012</fpage>
          <pub-id pub-id-type="doi">10.1093/jamiaopen/ooaf012</pub-id>
          <pub-id pub-id-type="pmid">40078164</pub-id>
        </element-citation>
      </ref>
      <ref id="B33-information-16-00894">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mondal</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Inamdar</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Seqmate: A novel large language model pipeline for automating rna sequencing</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2407.03381</pub-id>
        </element-citation>
      </ref>
      <ref id="B34-information-16-00894">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Qian</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Z.-A.</given-names>
            </name>
            <name>
              <surname>Tan</surname>
              <given-names>K.C.</given-names>
            </name>
          </person-group>
          <article-title>Qm-tot: A medical tree of thoughts reasoning framework for quantized model</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2504.12334</pub-id>
        </element-citation>
      </ref>
      <ref id="B35-information-16-00894">
        <label>35.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Luo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Qiao</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Long</surname>
              <given-names>Q.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Large language model agent: A survey on methodology, applications and challenges</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2503.21460</pub-id>
          <pub-id pub-id-type="arxiv">2503.21460</pub-id>
        </element-citation>
      </ref>
      <ref id="B36-information-16-00894">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Karma: Leveraging multi-agent llms for automated knowledge graph enrichment</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2502.06472</pub-id>
          <pub-id pub-id-type="arxiv">2502.06472</pub-id>
        </element-citation>
      </ref>
      <ref id="B37-information-16-00894">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jia</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Ddo: Dual-decision optimization via multi-agent collaboration for llm-based medical consultation</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2505.18630</pub-id>
        </element-citation>
      </ref>
      <ref id="B38-information-16-00894">
        <label>38.</label>
        <element-citation publication-type="thesis">
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Healthcare Agents: Large Language Models in Health Prediction and Decision-Making</article-title>
          <source>Ph.D. Dissertation</source>
          <publisher-name>Massachusetts Institute of Technology</publisher-name>
          <publisher-loc>Cambridge, MA, USA</publisher-loc>
          <year>2025</year>
        </element-citation>
      </ref>
      <ref id="B39-information-16-00894">
        <label>39.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Jeong</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>Y.S.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>McDuff</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ghassemi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Breazeal</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>H.W.</given-names>
            </name>
          </person-group>
          <article-title>Mdagents: An adaptive collaboration of llms for medical decision-making</article-title>
          <source>Adv. Neural Inf. Process. Syst.</source>
          <year>2024</year>
          <volume>37</volume>
          <fpage>79410</fpage>
          <lpage>79452</lpage>
        </element-citation>
      </ref>
      <ref id="B40-information-16-00894">
        <label>40.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Xiao</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>m-kailin: Knowledge-driven agentic scientific corpus distillation framework for biomedical large language models training</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2504.19565</pub-id>
        </element-citation>
      </ref>
      <ref id="B41-information-16-00894">
        <label>41.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cheng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Hong</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>J.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Exploring large language model based intelligent agents: Definitions, methods, and prospects</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2401.03428</pub-id>
          <pub-id pub-id-type="arxiv">2401.03428</pub-id>
        </element-citation>
      </ref>
      <ref id="B42-information-16-00894">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>McCoy</surname>
              <given-names>A.B.</given-names>
            </name>
            <name>
              <surname>Wright</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Improving large language model applications in biomedicine with retrieval-augmented generation: A systematic review, meta-analysis, and clinical development guidelines</article-title>
          <source>J. Am. Med Inform. Assoc.</source>
          <year>2025</year>
          <volume>32</volume>
          <fpage>ocaf008</fpage>
          <pub-id pub-id-type="doi">10.1093/jamia/ocaf008</pub-id>
        </element-citation>
      </ref>
      <ref id="B43-information-16-00894">
        <label>43.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Christophe</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kanithi</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Munjal</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Raha</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Hayat</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Rajan</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Al-Mahrooqi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Salman</surname>
              <given-names>M.U.</given-names>
            </name>
            <name>
              <surname>Pimentel</surname>
              <given-names>M.A.F.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Med42: Evaluating fine-tuning strategies for medical LLMs: Full parameter vs. parameter-efficient approaches</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2404.14779</pub-id>
        </element-citation>
      </ref>
      <ref id="B44-information-16-00894">
        <label>44.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Song</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wan</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>How to complete domain tuning while keeping general ability in llm: Adaptive layer-wise and element-wise regularization</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2501.13669</pub-id>
        </element-citation>
      </ref>
      <ref id="B45-information-16-00894">
        <label>45.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Khandekar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Xiong</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Dunn</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Applebaum</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Anwar</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Sarfo-Gyamfi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Safranek</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Anwar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>A.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Medcalc- bench: Evaluating large language models for medical calculations</article-title>
          <source>Adv. Neural Inf. Process. Syst.</source>
          <year>2024</year>
          <volume>37</volume>
          <fpage>84730</fpage>
          <lpage>84745</lpage>
        </element-citation>
      </ref>
      <ref id="B46-information-16-00894">
        <label>46.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Reflectool: To- wards reflection-aware tool-augmented clinical agents</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2410.17657</pub-id>
        </element-citation>
      </ref>
      <ref id="B47-information-16-00894">
        <label>47.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Medorch: Medical diagnosis with tool-augmented reasoning agents for flexible extensibility</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2506.00235</pub-id>
        </element-citation>
      </ref>
      <ref id="B48-information-16-00894">
        <label>48.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Z.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>MMedAgent: Learning to use medical tools with multi-modal agent</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2407.02483</pub-id>
        </element-citation>
      </ref>
      <ref id="B49-information-16-00894">
        <label>49.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Shi</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Zhuang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ho</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>M.D.</given-names>
            </name>
          </person-group>
          <article-title>EHRAgent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records</article-title>
          <source>Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing</source>
          <conf-loc>Miami, FL, USA</conf-loc>
          <conf-date>12&#x2013;16 November 2024</conf-date>
          <fpage>22315</fpage>
        </element-citation>
      </ref>
      <ref id="B50-information-16-00894">
        <label>50.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Choi</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Palumbo</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Chalasani</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Engelhard</surname>
              <given-names>M.M.</given-names>
            </name>
            <name>
              <surname>Jha</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Page</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>MALADE: Orchestration of LLM-powered agents with retrieval augmented generation for pharmacovigilance</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2408.01869</pub-id>
          <pub-id pub-id-type="arxiv">2408.01869</pub-id>
        </element-citation>
      </ref>
      <ref id="B51-information-16-00894">
        <label>51.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Niu</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Bi</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>L.K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>C.H.</given-names>
            </name>
            <name>
              <surname>Fei</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>From text to multimodality: Exploring the evolution and impact of large language models in medical practice</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2410.01812</pub-id>
        </element-citation>
      </ref>
      <ref id="B52-information-16-00894">
        <label>52.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wong</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Usuyama</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Naumann</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Poon</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Llava-med: Training a large language-and-vision assistant for biomedicine in one day</article-title>
          <source>Adv. Neural Inf. Process. Syst.</source>
          <year>2023</year>
          <volume>36</volume>
          <fpage>28541</fpage>
          <lpage>28564</lpage>
        </element-citation>
      </ref>
      <ref id="B53-information-16-00894">
        <label>53.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>W.J.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Geneverse: A collection of open-source multimodal large language models for genomic and proteomic research</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2406.15534</pub-id>
          <pub-id pub-id-type="arxiv">2406.15534</pub-id>
        </element-citation>
      </ref>
      <ref id="B54-information-16-00894">
        <label>54.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Bansal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dinh</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Pawar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Satishkumar</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Desai</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Medchat: A multi-agent framework for multimodal diagnosis with large language models</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2506.07400</pub-id>
        </element-citation>
      </ref>
      <ref id="B55-information-16-00894">
        <label>55.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ferber</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>El Nahhas</surname>
              <given-names>O.S.</given-names>
            </name>
            <name>
              <surname>Wo</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Wiest</surname>
              <given-names>I.C.</given-names>
            </name>
            <name>
              <surname>Clusmann</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Le&#xDF;mann</surname>
              <given-names>M.-E.</given-names>
            </name>
            <name>
              <surname>Foersch</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Lammert</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Tschochohei</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ja&#xA8;ger</surname>
              <given-names>D.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Development and validation of an autonomous artificial intelligence agent for clinical decision-making in oncology</article-title>
          <source>Nat. Cancer</source>
          <year>2025</year>
          <volume>6</volume>
          <fpage>1337</fpage>
          <lpage>1349</lpage>
          <pub-id pub-id-type="doi">10.1038/s43018-025-00991-6</pub-id>
        </element-citation>
      </ref>
      <ref id="B56-information-16-00894">
        <label>56.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gao</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Kong</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Noori</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ginder</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Tsiligkaridis</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zitnik</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Txagent: An ai agent for therapeutic reasoning across a universe of tools</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2503.10970</pub-id>
          <pub-id pub-id-type="arxiv">2503.10970</pub-id>
        </element-citation>
      </ref>
      <ref id="B57-information-16-00894">
        <label>57.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Tao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Git-mol: A multi-modal large language model for molecular science with graph, image, and text</article-title>
          <source>Comput. Biol. Med.</source>
          <year>2024</year>
          <volume>171</volume>
          <elocation-id>108073</elocation-id>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.108073</pub-id>
          <pub-id pub-id-type="pmid">38359660</pub-id>
        </element-citation>
      </ref>
      <ref id="B58-information-16-00894">
        <label>58.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>DrugAgent: Automating AI-aided drug discovery programming through LLM multi-agent collaboration</article-title>
          <source>Proceedings of the 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle</source>
          <conf-loc>Philadelphia, PA, USA</conf-loc>
          <conf-date>4 March 2025</conf-date>
        </element-citation>
      </ref>
      <ref id="B59-information-16-00894">
        <label>59.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Zhuang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhong</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>M.D.</given-names>
            </name>
            <name>
              <surname>Ruan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Medagentgym: Training llm agents for code-based medical reasoning at scale</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2506.04405</pub-id>
        </element-citation>
      </ref>
      <ref id="B60-information-16-00894">
        <label>60.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Sui</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Harrison</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>C.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Colacare: Enhancing electronic health record modeling through large language model-driven multi-agent collaboration</article-title>
          <source>Proc. ACM Web Conf.</source>
          <year>2025</year>
          <volume>2025</volume>
          <fpage>2250</fpage>
          <lpage>2261</lpage>
        </element-citation>
      </ref>
      <ref id="B61-information-16-00894">
        <label>61.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yi</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Albert</surname>
              <given-names>M.V.</given-names>
            </name>
          </person-group>
          <article-title>A multimodal multi-agent framework for radiology report generation</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2505.09787</pub-id>
          <pub-id pub-id-type="arxiv">2505.09787</pub-id>
        </element-citation>
      </ref>
      <ref id="B62-information-16-00894">
        <label>62.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Almansoori</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Cholakkal</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Self-evolving multi-agent simulations for realistic clinical interactions</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2503.22678</pub-id>
        </element-citation>
      </ref>
      <ref id="B63-information-16-00894">
        <label>63.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jiang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Black</surname>
              <given-names>K.C.</given-names>
            </name>
            <name>
              <surname>Geng</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ng</surname>
              <given-names>A.Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J.H.</given-names>
            </name>
          </person-group>
          <article-title>Medagentbench: Dataset for benchmarking llms as agents in medical applications</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2501.14654</pub-id>
        </element-citation>
      </ref>
      <ref id="B64-information-16-00894">
        <label>64.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mehandru</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Miao</surname>
              <given-names>B.Y.</given-names>
            </name>
            <name>
              <surname>Almaraz</surname>
              <given-names>E.R.</given-names>
            </name>
            <name>
              <surname>Sushil</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Butte</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Alaa</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Evaluating large language models as agents in the clinic</article-title>
          <source>NPJ Digit. Med.</source>
          <year>2024</year>
          <volume>7</volume>
          <fpage>84</fpage>
          <pub-id pub-id-type="doi">10.1038/s41746-024-01083-y</pub-id>
        </element-citation>
      </ref>
      <ref id="B65-information-16-00894">
        <label>65.</label>
        <element-citation publication-type="web">
          <person-group person-group-type="author">
            <name>
              <surname>Johnson</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bulgarelli</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Pollard</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Horng</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Celi</surname>
              <given-names>L.A.</given-names>
            </name>
            <name>
              <surname>Mark</surname>
              <given-names>R.</given-names>
            </name>
            <collab>Mimic-iv</collab>
          </person-group>
          <article-title>PhysioNet</article-title>
          <year>2020</year>
          <fpage>49</fpage>
          <lpage>55</lpage>
          <comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://physionet.org/content/mimiciv/1.0/" ext-link-type="uri">https://physionet.org/content/mimiciv/1.0/</ext-link></comment>
          <date-in-citation content-type="access-date">(accessed on 23 August 2021)</date-in-citation>
        </element-citation>
      </ref>
      <ref id="B66-information-16-00894">
        <label>66.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Song</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Trotter</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J.Y.</given-names>
            </name>
          </person-group>
          <article-title>Llm agent swarm for hypothesis- driven drug discovery</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2504.17967</pub-id>
          <pub-id pub-id-type="arxiv">2504.17967</pub-id>
        </element-citation>
      </ref>
      <ref id="B67-information-16-00894">
        <label>67.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mehenni</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Zouaq</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Medhal: An evaluation dataset for medical hallucination detection</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2504.08596</pub-id>
          <pub-id pub-id-type="arxiv">2504.08596</pub-id>
        </element-citation>
      </ref>
      <ref id="B68-information-16-00894">
        <label>68.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mitchener</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Laurent</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Tenmann</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Narayanan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wellawatte</surname>
              <given-names>G.P.</given-names>
            </name>
            <name>
              <surname>White</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sani</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Rodriques</surname>
              <given-names>S.G.</given-names>
            </name>
          </person-group>
          <article-title>Bixbench: A comprehensive benchmark for llm-based agents in computational biology</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2503.00096</pub-id>
        </element-citation>
      </ref>
      <ref id="B69-information-16-00894">
        <label>69.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fan</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>AI Hospital: Benchmarking large language models in a multi-agent medical interaction simulator</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2402.09742</pub-id>
        </element-citation>
      </ref>
      <ref id="B70-information-16-00894">
        <label>70.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Altermatt</surname>
              <given-names>F.R.</given-names>
            </name>
            <name>
              <surname>Neyem</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sumonte</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Mendoza</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Villagran</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Lacassie</surname>
              <given-names>H.J.</given-names>
            </name>
          </person-group>
          <article-title>Performance of single-agent and multi-agent language models in spanish language medical competency exams</article-title>
          <source>BMC Med. Educ.</source>
          <year>2025</year>
          <volume>25</volume>
          <elocation-id>666</elocation-id>
          <pub-id pub-id-type="doi">10.1186/s12909-025-07250-3</pub-id>
        </element-citation>
      </ref>
      <ref id="B71-information-16-00894">
        <label>71.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>A layered debating multi-agent system for similar disease diagnosis</article-title>
          <source>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)</source>
          <conf-loc>Albuquerque, NM, USA</conf-loc>
          <conf-date>29 April&#x2013;4 May 2025</conf-date>
          <fpage>539</fpage>
          <lpage>549</lpage>
        </element-citation>
      </ref>
      <ref id="B72-information-16-00894">
        <label>72.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Schaekermann</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Palepu</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Saab</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Freyberg</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Tanno</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Amin</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>Y.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Towards conversational diagnostic artificial intelligence</article-title>
          <source>Nature</source>
          <year>2025</year>
          <volume>642</volume>
          <fpage>442</fpage>
          <lpage>450</lpage>
          <pub-id pub-id-type="doi">10.1038/s41586-025-08866-7</pub-id>
        </element-citation>
      </ref>
      <ref id="B73-information-16-00894">
        <label>73.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Garcia-Fernandez</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Felipe</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Shotande</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zitu</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Tripathi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rasool</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Naqa</surname>
              <given-names>I.E.</given-names>
            </name>
            <name>
              <surname>Rudrapatna</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Valdes</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Trustworthy ai for medicine: Continuous hallucination detection and elimination with check</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2506.11129</pub-id>
        </element-citation>
      </ref>
      <ref id="B74-information-16-00894">
        <label>74.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bunnell</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Bondy</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Fromtling</surname>
              <given-names>L.M.</given-names>
            </name>
            <name>
              <surname>Ludeman</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Gourab</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Bridging ai and healthcare: A scoping review of retrieval- augmented generation&#x2014;Ethics, bias, transparency, improvements, and applications</article-title>
          <source>medRxiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.1101/2025.04.01.25325033</pub-id>
        </element-citation>
      </ref>
      <ref id="B75-information-16-00894">
        <label>75.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Rani</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Mishra</surname>
              <given-names>B.K.</given-names>
            </name>
            <name>
              <surname>Thakker</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Babar</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Din</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Biases and trustworthiness challenges with mitigation strategies for large language models in healthcare</article-title>
          <source>Proceedings of the 2024 International Conference on IT and Industrial Technologies (ICIT)</source>
          <conf-loc>Chiniot, Pakistan</conf-loc>
          <conf-date>10&#x2013;12 December 2024</conf-date>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Piscataway, NJ, USA</publisher-loc>
          <year>2024</year>
          <fpage>1</fpage>
          <lpage>6</lpage>
        </element-citation>
      </ref>
      <ref id="B76-information-16-00894">
        <label>76.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gui</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wan</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Cod, towards an interpretable medical agent using chain of diagnosis</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2407.13301</pub-id>
          <pub-id pub-id-type="arxiv">2407.13301</pub-id>
        </element-citation>
      </ref>
      <ref id="B77-information-16-00894">
        <label>77.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Goktas</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Grzybowski</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Shaping the future of healthcare: Ethical clinical challenges and pathways to trustworthy ai</article-title>
          <source>J. Clin. Med.</source>
          <year>2025</year>
          <volume>14</volume>
          <elocation-id>1605</elocation-id>
          <pub-id pub-id-type="doi">10.3390/jcm14051605</pub-id>
        </element-citation>
      </ref>
      <ref id="B78-information-16-00894">
        <label>78.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jin</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Wright</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Wilbur</surname>
              <given-names>W.J.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Taylor</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Q.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2402.13225</pub-id>
        </element-citation>
      </ref>
      <ref id="B79-information-16-00894">
        <label>79.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Comeau</surname>
              <given-names>D.S.</given-names>
            </name>
            <name>
              <surname>Bitterman</surname>
              <given-names>D.S.</given-names>
            </name>
            <name>
              <surname>Celi</surname>
              <given-names>L.A.</given-names>
            </name>
          </person-group>
          <article-title>Preventing unrestricted and unmonitored ai experimentation in healthcare through transparency and accountability</article-title>
          <source>npj Digit. Med.</source>
          <year>2025</year>
          <volume>8</volume>
          <fpage>42</fpage>
          <pub-id pub-id-type="doi">10.1038/s41746-025-01443-2</pub-id>
        </element-citation>
      </ref>
      <ref id="B80-information-16-00894">
        <label>80.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Y.C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Law</surname>
              <given-names>J.N.</given-names>
            </name>
            <name>
              <surname>Murali</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Pandey</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Integrating multimodal data through interpretable heterogeneous ensembles</article-title>
          <source>Bioinform. Adv.</source>
          <year>2022</year>
          <volume>2</volume>
          <elocation-id>vbac065</elocation-id>
          <pub-id pub-id-type="doi">10.1093/bioadv/vbac065</pub-id>
        </element-citation>
      </ref>
      <ref id="B81-information-16-00894">
        <label>81.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>AlSaad</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Alrazaq</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Boughorbel</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ahmed</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Renault</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Damseh</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sheikh</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Multimodal large language models in health care: Applications, challenges, and future outlook</article-title>
          <source>J. Med. Internet Res.</source>
          <year>2024</year>
          <volume>26</volume>
          <fpage>e59505</fpage>
          <pub-id pub-id-type="doi">10.2196/59505</pub-id>
        </element-citation>
      </ref>
      <ref id="B82-information-16-00894">
        <label>82.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nweke</surname>
              <given-names>I.P.</given-names>
            </name>
            <name>
              <surname>Ogadah</surname>
              <given-names>C.O.</given-names>
            </name>
            <name>
              <surname>Koshechkin</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Oluwasegun</surname>
              <given-names>P.M.</given-names>
            </name>
          </person-group>
          <article-title>Multi-agent ai systems in healthcare: A systematic review enhancing clinical decision-making</article-title>
          <source>Asian J. Med. Princ. Clin. Pract.</source>
          <year>2025</year>
          <volume>8</volume>
          <fpage>273</fpage>
          <lpage>285</lpage>
          <pub-id pub-id-type="doi">10.9734/ajmpcp/2025/v8i1288</pub-id>
        </element-citation>
      </ref>
      <ref id="B83-information-16-00894">
        <label>83.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pandey</surname>
              <given-names>H.G.</given-names>
            </name>
            <name>
              <surname>Amod</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Advancing healthcare automation: Multi-agent system for medical necessity justification</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2404.17977</pub-id>
          <pub-id pub-id-type="arxiv">2404.17977</pub-id>
        </element-citation>
      </ref>
      <ref id="B84-information-16-00894">
        <label>84.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wei</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Medco: Medical education copilots based on a multi-agent framework</article-title>
          <source>European Conference on Computer Vision</source>
          <publisher-name>Springer</publisher-name>
          <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
          <year>2025</year>
          <fpage>119</fpage>
          <lpage>135</lpage>
        </element-citation>
      </ref>
      <ref id="B85-information-16-00894">
        <label>85.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nov</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Aphinyanaphongs</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lui</surname>
              <given-names>Y.W.</given-names>
            </name>
            <name>
              <surname>Mann</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Porfiri</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Riedl</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rizzo</surname>
              <given-names>J.-R.</given-names>
            </name>
            <name>
              <surname>Wiesenfeld</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>The transformation of patient-clinician relationships with ai-based medical advice</article-title>
          <source>Commun. ACM</source>
          <year>2021</year>
          <volume>64</volume>
          <fpage>46</fpage>
          <lpage>48</lpage>
          <pub-id pub-id-type="doi">10.1145/3417518</pub-id>
        </element-citation>
      </ref>
      <ref id="B86-information-16-00894">
        <label>86.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhen</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Huo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Medsentry: Understanding and mitigating safety risks in medical llm multi-agent systems</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2505.20824</pub-id>
        </element-citation>
      </ref>
      <ref id="B87-information-16-00894">
        <label>87.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pham</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Ethical and legal considerations in healthcare ai: Innovation and policy for safe and fair use</article-title>
          <source>R. Soc. Open Sci.</source>
          <year>2025</year>
          <volume>12</volume>
          <fpage>241873</fpage>
          <pub-id pub-id-type="doi">10.1098/rsos.241873</pub-id>
        </element-citation>
      </ref>
      <ref id="B88-information-16-00894">
        <label>88.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cheong</surname>
              <given-names>B.C.</given-names>
            </name>
          </person-group>
          <article-title>Transparency and accountability in ai systems: Safe- guarding wellbeing in the age of algorithmic decision-making</article-title>
          <source>Front. Hum. Dyn.</source>
          <year>2024</year>
          <volume>6</volume>
          <elocation-id>1421273</elocation-id>
          <pub-id pub-id-type="doi">10.3389/fhumd.2024.1421273</pub-id>
        </element-citation>
      </ref>
      <ref id="B89-information-16-00894">
        <label>89.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Palaniappan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>E.Y.T.</given-names>
            </name>
            <name>
              <surname>Vogel</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Global regulatory frameworks for the use of artificial intelligence (ai) in the healthcare services sector</article-title>
          <source>Healthcare</source>
          <year>2024</year>
          <volume>12</volume>
          <elocation-id>562</elocation-id>
          <pub-id pub-id-type="doi">10.3390/healthcare12050562</pub-id>
        </element-citation>
      </ref>
      <ref id="B90-information-16-00894">
        <label>90.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xi</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Zodiac: A cardiologist-level llm framework for multi-agent diagnostics</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="arxiv">2410.02026</pub-id>
        </element-citation>
      </ref>
      <ref id="B91-information-16-00894">
        <label>91.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Swanson</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Bulaong</surname>
              <given-names>N.L.</given-names>
            </name>
            <name>
              <surname>Pak</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>The virtual lab: Ai agents design new SARS-CoV-2 nanobodies with experimental validation</article-title>
          <source>bioRxiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.1101/2024.11.11.623004</pub-id>
        </element-citation>
      </ref>
      <ref id="B92-information-16-00894">
        <label>92.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cui</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Shao</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ho</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Llms-based few-shot disease predictions using ehr: A novel approach combining predictive agent reasoning and critical agent instruction</article-title>
          <source>AMIA Annu. Symp. Proc.</source>
          <year>2024</year>
          <volume>2025</volume>
          <fpage>319</fpage>
        </element-citation>
      </ref>
      <ref id="B93-information-16-00894">
        <label>93.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Low</surname>
              <given-names>C.H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Medagent-pro: Towards evidence-based multi-modal medical diagno- sis via reasoning agentic workflow</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2503.18968</pub-id>
        </element-citation>
      </ref>
      <ref id="B94-information-16-00894">
        <label>94.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Bai</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bian</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Autonomous multi-modal llm agents for treatment planning in focused ultrasound ablation surgery</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2505.21418</pub-id>
          <pub-id pub-id-type="arxiv">2505.21418</pub-id>
        </element-citation>
      </ref>
      <ref id="B95-information-16-00894">
        <label>95.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mo&#xEB;ll</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Aronsson</surname>
              <given-names>F.S.</given-names>
            </name>
            <name>
              <surname>Akbar</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Medical reasoning in LLMs: An in-depth analysis of DeepSeek R1</article-title>
          <source>Front. Artif. Intell.</source>
          <year>2025</year>
          <volume>8</volume>
          <elocation-id>1616145</elocation-id>
          <pub-id pub-id-type="doi">10.3389/frai.2025.1616145</pub-id>
        </element-citation>
      </ref>
      <ref id="B96-information-16-00894">
        <label>96.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Matsumoto</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Moran</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hernandez</surname>
              <given-names>M.E.</given-names>
            </name>
            <name>
              <surname>Venkatesan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>J.-H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Moore</surname>
              <given-names>J.H.</given-names>
            </name>
          </person-group>
          <article-title>Escargot: An ai agent leveraging large language models, dynamic graph of thoughts, and biomedical knowledge graphs for enhanced reasoning</article-title>
          <source>Bioinformatics</source>
          <year>2025</year>
          <volume>41</volume>
          <elocation-id>btaf031</elocation-id>
          <pub-id pub-id-type="doi">10.1093/bioinformatics/btaf031</pub-id>
        </element-citation>
      </ref>
      <ref id="B97-information-16-00894">
        <label>97.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhai</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Mragent: An llm-based automated agent for causal knowledge discovery in disease via mendelian randomization</article-title>
          <source>Brief. Bioinform.</source>
          <year>2025</year>
          <volume>26</volume>
          <elocation-id>bbaf140</elocation-id>
          <pub-id pub-id-type="doi">10.1093/bib/bbaf140</pub-id>
        </element-citation>
      </ref>
      <ref id="B98-information-16-00894">
        <label>98.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Atf</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Safavi-Naini</surname>
              <given-names>S.A.A.</given-names>
            </name>
            <name>
              <surname>Lewis</surname>
              <given-names>P.R.</given-names>
            </name>
            <name>
              <surname>Mahjoubfar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Naderi</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Savage</surname>
              <given-names>T.R.</given-names>
            </name>
            <name>
              <surname>Soroush</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>The challenge of uncertainty quan- tification of large language models in medicine</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="arxiv">2504.05278</pub-id>
        </element-citation>
      </ref>
      <ref id="B99-information-16-00894">
        <label>99.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhuang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Learning to be a doctor: Searching for effective medical agent architectures</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2504.11301</pub-id>
          <pub-id pub-id-type="arxiv">2504.11301</pub-id>
        </element-citation>
      </ref>
      <ref id="B100-information-16-00894">
        <label>100.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zheng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <article-title>Lifelong learning of large language model based agents: A roadmap</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2501.07278</pub-id>
          <pub-id pub-id-type="arxiv">2501.07278</pub-id>
        </element-citation>
      </ref>
      <ref id="B101-information-16-00894">
        <label>101.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.-Q.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>W.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Agent hospital: A simulacrum of hospital with evolvable medical agents</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2405.02957</pub-id>
          <pub-id pub-id-type="arxiv">2405.02957</pub-id>
        </element-citation>
      </ref>
      <ref id="B102-information-16-00894">
        <label>102.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Casaletto</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Foley</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Fernandez</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Sanders</surname>
              <given-names>L.M.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>R.T.</given-names>
            </name>
            <name>
              <surname>Ranjan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jain</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Haynes</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Boerma</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Costes</surname>
              <given-names>S.V.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Foundational architecture enabling federated learning for training space biomedical machine learning models between the international space station and earth</article-title>
          <source>bioRxiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.1101/2025.01.14.633017</pub-id>
        </element-citation>
      </ref>
      <ref id="B103-information-16-00894">
        <label>103.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Federated learning with layer skipping: Efficient training of large language models for healthcare nlp</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2504.10536</pub-id>
          <pub-id pub-id-type="arxiv">2504.10536</pub-id>
        </element-citation>
      </ref>
      <ref id="B104-information-16-00894">
        <label>104.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rosenthal</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Beecy</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sabuncu</surname>
              <given-names>M.R.</given-names>
            </name>
          </person-group>
          <article-title>Rethinking clinical trials for medical ai with dynamic deployments of adaptive systems</article-title>
          <source>npj Digit. Med.</source>
          <year>2025</year>
          <volume>8</volume>
          <fpage>252</fpage>
          <pub-id pub-id-type="doi">10.1038/s41746-025-01674-3</pub-id>
          <pub-id pub-id-type="pmid">40328886</pub-id>
        </element-citation>
      </ref>
      <ref id="B105-information-16-00894">
        <label>105.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bedi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Orr-Ewing</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Dash</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Koyejo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Callahan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Fries</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Wornow</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Swaminathan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lehmann</surname>
              <given-names>L.S.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A systematic review of testing and evaluation of healthcare applications of large language models (llms)</article-title>
          <source>medRxiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.1101/2024.04.15.24305869</pub-id>
        </element-citation>
      </ref>
      <ref id="B106-information-16-00894">
        <label>106.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Solaiman</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Mekki</surname>
              <given-names>Y.M.</given-names>
            </name>
            <name>
              <surname>Qadir</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ghaly</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Abdelkareem</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Al-Ansari</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>A &#x201C;true lifecycle approach&#x201D; towards governing healthcare ai with the gcc as a global governance model</article-title>
          <source>npj Digit. Med.</source>
          <year>2025</year>
          <volume>8</volume>
          <fpage>337</fpage>
          <pub-id pub-id-type="doi">10.1038/s41746-025-01614-1</pub-id>
          <pub-id pub-id-type="pmid">40473925</pub-id>
        </element-citation>
      </ref>
      <ref id="B107-information-16-00894">
        <label>107.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sankar</surname>
              <given-names>B.S.</given-names>
            </name>
            <name>
              <surname>Gilliland</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Rincon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hermjakob</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Adam</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Lemaster</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Watson</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Bui</surname>
              <given-names>A.</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Building an ethical and trustworthy biomedical ai ecosystem for the translational and clinical integration of foundation models</article-title>
          <source>Bioengineering</source>
          <year>2024</year>
          <volume>11</volume>
          <elocation-id>984</elocation-id>
          <pub-id pub-id-type="doi">10.3390/bioengineering11100984</pub-id>
          <pub-id pub-id-type="pmid">39451360</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <sec sec-type="display-objects">
      <title>Figures and Tables</title>
      <fig id="information-16-00894-f001" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Overview of the literature search and screening process for biomedical LLM agent studies included in this review.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="information-16-00894-g001.tif"/>
      </fig>
      <fig id="information-16-00894-f002" position="float">
        <label>Figure 2</label>
        <caption>
          <p>A conceptual landscape of biomedical LLM agents. The architecture spans from agentic core components to system architectures, key enabling techniques, and real-world applications.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="information-16-00894-g002.tif"/>
      </fig>
      <fig id="information-16-00894-f003" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Key challenges and mitigation strategies for biomedical LLM agents.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="information-16-00894-g003.tif"/>
      </fig>
      <fig id="information-16-00894-f004" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Comprehensive workflows and use cases of biomedical LLM agents.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="information-16-00894-g004.tif"/>
      </fig>
      <table-wrap id="information-16-00894-t001" position="float">
        <object-id pub-id-type="pii">information-16-00894-t001_Table 1</object-id>
        <label>Table 1</label>
        <caption>
          <p>Representative Biomedical LLM Agents and Frameworks.</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Agent/Framework</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Core LLM</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Key Methods</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Application</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Ref.</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="middle">MedAgents</td>
              <td align="left" valign="middle">GPT-4, GPT-3.5</td>
              <td align="left" valign="middle">MAS, RAG (implicit), CoT</td>
              <td align="left" valign="middle">Medical reasoning</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B21-information-16-00894">21</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">MAC Framework</td>
              <td align="left" valign="middle">GPT-4, GPT-3.5</td>
              <td align="left" valign="middle">MAS (MDT simulation)</td>
              <td align="left" valign="middle">Rare disease diagnosis</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B22-information-16-00894">22</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">KG4Diagnosis</td>
              <td align="left" valign="middle">GPT, MedPaLM</td>
              <td align="left" valign="middle">MAS, RAG, Tool Use</td>
              <td align="left" valign="middle">Diagnosis w/KG</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B23-information-16-00894">23</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">BioResearcher</td>
              <td align="left" valign="middle">GPT-4o</td>
              <td align="left" valign="middle">MAS, Literature</td>
              <td align="left" valign="middle">Automated research</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B11-information-16-00894">11</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">CT-Agent</td>
              <td align="left" valign="middle">GPT-4</td>
              <td align="left" valign="middle">MAS, ReAct, Tool Use</td>
              <td align="left" valign="middle">Clinical trial analysis</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B24-information-16-00894">24</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">CRISPR-GPT</td>
              <td align="left" valign="middle">N/A</td>
              <td align="left" valign="middle">Tool Use, Planning</td>
              <td align="left" valign="middle">Gene editing design</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B25-information-16-00894">25</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">BioDiscoveryAgent</td>
              <td align="left" valign="middle">Claude 3.5</td>
              <td align="left" valign="middle">PubMed Tool, Critic Agent</td>
              <td align="left" valign="middle">Perturbation planning</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B26-information-16-00894">26</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">GeneSilico Copilot</td>
              <td align="left" valign="middle">N/A</td>
              <td align="left" valign="middle">RAG, Retrieval Tools, ReAct</td>
              <td align="left" valign="middle">Oncology (breast cancer)</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B27-information-16-00894">27</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">ArgMed-Agents</td>
              <td align="left" valign="middle">GPT-3.5, GPT-4</td>
              <td align="left" valign="middle">MAS, Symbolic Reasoning</td>
              <td align="left" valign="middle">Explainable decision making</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B28-information-16-00894">28</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">ENTAgents</td>
              <td align="left" valign="middle">N/A</td>
              <td align="left" valign="middle">MAS, RAG, Reflection</td>
              <td align="left" valign="middle">ENT QA system</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B29-information-16-00894">29</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">Taiyi</td>
              <td align="left" valign="middle">Qwen-7B-base</td>
              <td align="left" valign="middle">Bilingual Fine-tuning</td>
              <td align="left" valign="middle">Biomed NLP tasks</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B30-information-16-00894">30</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">MedBioLM</td>
              <td align="left" valign="middle">N/A</td>
              <td align="left" valign="middle">Fine-tuning, RAG</td>
              <td align="left" valign="middle">Biomedical QA</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B31-information-16-00894">31</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">LLM-IE Agent</td>
              <td align="left" valign="middle">N/A</td>
              <td align="left" valign="middle">Prompt Editor Tool Use</td>
              <td align="left" valign="middle">Biomedical IE (NER, RE)</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B32-information-16-00894">32</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">SeqMate</td>
              <td align="left" valign="middle">gpt-3.5-turbo</td>
              <td align="left" valign="middle">BioTools, Planning</td>
              <td align="left" valign="middle">RNA-seq analysis</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B33-information-16-00894">33</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">Clinical Calc Agent</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">LLaMa, GPT-4o</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Code Tool, RAG, API</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Clinical scoring tasks</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">[<xref ref-type="bibr" rid="B4-information-16-00894">4</xref>]</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>Note: MAS = Multi-Agent System; RAG = Retrieval-Augmented Generation; CoT = Chain-of-Thought; MDT = Multidisciplinary Team; API = Application Programming Interface; ENT = Ear, Nose, and Throat (otolaryngology); NLP = Natural Language Processing; IE = Information Extraction; NER = Named Entity Recognition; RE = Relation Extraction. &#x201C;Core LLM&#x201D; refers to the foundational model; actual implementations may vary or be further fine-tuned.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="information-16-00894-t002" position="float">
        <object-id pub-id-type="pii">information-16-00894-t002_Table 2</object-id>
        <label>Table 2</label>
        <caption>
          <p>Biomedical LLM Agent Evaluation Benchmarks.</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Benchmark</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Focus</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Modalities</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Metrics</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Ref.</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="middle">AgentClinic</td>
              <td align="left" valign="middle">Clinical simulation</td>
              <td align="left" valign="middle">Dialogue, Image, EHR</td>
              <td align="left" valign="middle">Accuracy, Bias analysis</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B8-information-16-00894">8</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">MedJourney</td>
              <td align="left" valign="middle">Patient journey QA</td>
              <td align="left" valign="middle">Dialogue, Text</td>
              <td align="left" valign="middle">Accuracy, BLEU, Recall</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B7-information-16-00894">7</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">CalcQA</td>
              <td align="left" valign="middle">Tool-based execution</td>
              <td align="left" valign="middle">Text (cases)</td>
              <td align="left" valign="middle">Tool accuracy</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B6-information-16-00894">6</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">MedAgentBench</td>
              <td align="left" valign="middle">FHIR interaction</td>
              <td align="left" valign="middle">Text + EHR</td>
              <td align="left" valign="middle">Task success rate</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B63-information-16-00894">63</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">MedHal</td>
              <td align="left" valign="middle">Hallucination detection</td>
              <td align="left" valign="middle">Text (clinical, QA)</td>
              <td align="left" valign="middle">Accuracy, F1 score</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B67-information-16-00894">67</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle">SourceCheckup</td>
              <td align="left" valign="middle">Citation verification</td>
              <td align="left" valign="middle">Text, URL</td>
              <td align="left" valign="middle">Source accuracy</td>
              <td align="left" valign="middle">[<xref ref-type="bibr" rid="B5-information-16-00894">5</xref>]</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">BixBench</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Bioinformatics QA</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Text + Bio data</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Task accuracy</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">[<xref ref-type="bibr" rid="B68-information-16-00894">68</xref>]</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>Note: EHR = Electronic Health Record; FHIR = Fast Healthcare Interoperability Resources; QA = Question Answering; BLEU = Bilingual Evaluation Understudy (machine-translation metric); F1 = harmonic mean of precision and recall; URL = Uniform Resource Locator.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="information-16-00894-t003" position="float">
        <object-id pub-id-type="pii">information-16-00894-t003_Table 3</object-id>
        <label>Table 3</label>
        <caption>
          <p>Challenges&#x2013;Mitigations&#x2013;Metrics for Biomedical LLM Agents.</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Challenge</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Impact</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Current Mitigation</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Limitations</th>
              <th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Proposed Metric</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">Hallucinations and Factual<break/>Inaccuracies</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Misdiagnoses, fabricated<break/>references, unverified outputs</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">RAG, tool-based computation, self-correction, prompt engineering</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Residual factual errors, difficulty defining, hallucination boundaries</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Hallucination Trust Index (HTI)</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">Explainability and Transparency</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Limited&#xA0;interpretability,&#xA0;clinician&#xA0;distrust&#xA0;</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">SHAP, LIME, CoT, CoD, argumentation, model cards, human-in-the-loop</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Opaque internal logic, difficult<break/>for non-experts to audit outputs</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">&#x2014;</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">Data Quality, Availability, and Bias</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Biased predictions, reduced generalization</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Dataset&#xA0;curation,&#xA0;bias&#xA0;audits,&#xA0;fairness-aware&#xA0;training&#xA0;</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Limited real-world demographic coverage</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Equity Alignment Score (EAS)</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">Tool Reliability and Integration</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Calculation errors, API misuse</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Prompt refinement, toolspecific<break/>evaluation, error handling</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Interface mismatch, invocation<break/>confusion</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Tool Execution Fidelity (TEF)</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">Multimodal Data Integration and Processing</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Sparse or conflicting patient signals</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Joint encoders, gated fusion, alignment protocols</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Modality gaps, noisy inputs</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Multimodal Alignment<break/>Consistency (MAC)</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">Multi-Agent Collaboration Complexity</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Inter-agent misalignment, redundant roles</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Role assignment, consensus mechanisms</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Coordination latency,<break/>planning inconsistencies</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Agent Coordination Latency (ACL)</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin">Ethics, Privacy, Security, Regulation</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Data misuse, legal ambiguity, safety concerns</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">Privacy-preserving computation,<break/>AI-specific governance</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">No agent-specific regulatory<break/>standard, unclear responsibility</td>
              <td align="left" valign="middle" style="border-bottom:solid thin">AI Governance Readiness<break/>Index (AGRI)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <fn-group>
      <fn>
        <p><bold>Disclaimer/Publisher&#x2019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p>
      </fn>
    </fn-group>
  </back>
</article>
