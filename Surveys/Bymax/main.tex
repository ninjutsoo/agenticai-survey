% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{acl}
\usepackage[]{acl}

% My packages for the table
\usepackage{multicol}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\raisebox{-1.5ex}{\includegraphics[width=1cm]{baymax.png}}A Survey of LLM-based Agents in Medicine:  \\  How far are we from Baymax?}


\author{Wenxuan Wang$^{1}$\thanks{~~Wenxuan Wang and Zizhan Ma are equal contribute to this paper.} \quad Zizhan Ma$^{1}$$^*$ \quad Zheng Wang$^1$ \quad Chenghan Wu $^{1}$ \\ \bf \bf Jiaming Ji$^{2}$ \quad \bf Wenting Chen$^{3}$  \quad  \bf Xiang Li$^4$  \quad \bf Yixuan Yuan$^1$ \\
$^1$The Chinese University of Hong Kong  \\
$^2$ Peking University  \quad \quad  $^3$ City University of Hong Kong\\
$^4$ Massachusetts General Hospital and Harvard Medical School\\
$^1$\texttt{\{wenxuanwang,zizhan.ma\}@link.cuhk.edu.hk}   \quad
$^2$\texttt{wentichen7-c@my.cityu.edu.hk} \\ 
%$^3$\texttt{youliangyuan@link.cuhk.edu.cn}\\
}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\begin{abstract}
% Large Language Models (LLMs) have demonstrated significant potential in medical applications through their advanced natural language understanding and generation capabilities. This survey examines the current state of LLM-based agents in medical applications, analyzing their architectures, functionalities, and impact across various healthcare domains. We systematically review recent developments in medical agent frameworks, including their approaches to data processing, knowledge integration, reasoning strategies, and multi-agent collaboration. The survey covers key application scenarios, evaluation frameworks, and current challenges such as data privacy, multimodal integration, and hallucination risks. Through this comprehensive analysis, we aim to provide researchers and practitioners with insights into the current landscape and future prospects of LLM-based medical agents.


% short version
% Large Language Models (LLMs) are transforming healthcare through LLM-based agents that can understand and assist with medical tasks. This survey examines the architectures, applications, and challenges of LLM-based agents in medicine. We analyze key components including system profiles, clinical planning, medical reasoning frameworks, and external capacity enhancement. The survey covers major applications in clinical decision support, medical documentation, training simulations, and healthcare service optimization, along with evaluation frameworks and metrics. While these agents show promise in enhancing healthcare delivery, challenges remain in hallucination management, multimodal integration, implementation, and ethics. We conclude by highlighting future directions in medical reasoning, physical system integration, and training simulations, providing researchers and practitioners with a structured overview of the field's current state and prospects.
Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.

\end{abstract}

\iffalse
Recent advancements in large language models (LLMs) have redefined the capabilities of artificial intelligence, achieving significant improvements in natural language understanding, generation, and reasoning \cite{10.1145/3490099.3511105}. In medicine, LLMs have been successfully applied to clinical decision support, diagnostic assistance, patient communication, and medical education \cite{kim2024mdagentsadaptivecollaborationllms,mukherjee2024polarissafetyfocusedllmconstellation,yu2024aipatientsimulatingpatientsehrs}. By leveraging structured medical knowledge bases and clinical guidelines, researchers have developed sophisticated systems that provide evidence-based recommendations and streamline complex clinical workflows \cite{wei2024medaideomnimedicalaide,tang2024medagentslargelanguagemodels}. For instance, MedPaLM has achieved competitive performance in medical licensing examinations, highlighting the potential for deploying LLMs in high-stakes healthcare environments \cite{singhal2023expertlevelmedicalquestionanswering}.

In parallel with the extensive research on LLMs in medicine, a novel class of systems—LLM-based agents—has emerged. Unlike conventional LLMs that primarily process and generate text, these agents are designed to act autonomously by incorporating multi-step reasoning, external tool invocation, and coordinated multi-agent collaboration \cite{Mehandru2024EvaluatingLL}. Such agents are capable not only of comprehending clinical language but also of interacting dynamically with external databases and specialized tools to address complex, real-world clinical scenarios. Although several reviews have synthesized applications of LLMs in healthcare \cite{zheng2024largelanguagemodelsmedicine,chen2024evaluatinglargelanguagemodels,yuan2023largelanguagemodelsilluminate}, there remains a notable gap regarding dedicated surveys on LLM-based agents for medical applications.

Our motivation for this survey arises from the recognition that LLM-based agents possess unique characteristics—such as explicit task orientation, integrated system design, and dynamic decision-making—that make them especially well-suited for the data-intensive and multifaceted environment of healthcare. Understanding the architectures, practical implementations, evaluation methodologies, and limitations of these agents is essential for advancing their safe and effective integration into clinical practice.

To compile this review, we conducted a systematic literature search across major academic databases (including PubMed, ACM Digital Library, arXiv, and Google Scholar) using keywords such as “large language model,” “medical agent,” “clinical decision support,” and “healthcare AI.” The search was restricted to publications from 2022 to 2024, yielding approximately 300 initial articles. After screening titles and abstracts for relevance and quality, a full-text review of 80 shortlisted papers was performed, ultimately selecting about 60 studies that specifically address LLM-based agents in medical contexts.

Despite promising progress, challenges remain in ensuring the reliability, safety, and ethical use of these systems in clinical practice \cite{sun2024applications,yuan-etal-2024-r,yan2025application}. Issues such as data privacy, management of hallucinations, and the integration of multimodal inputs require further investigation. This survey provides a structured analysis of the current state of LLM-based agents in medical applications, outlining their architectures, capabilities, and evaluation methodologies while identifying key challenges that need to be addressed in future research.

The remainder of this paper is organized as follows. Section~\ref{sec:background} provides a comprehensive background on LLM-based agents, detailing their evolution and highlighting the differences from traditional LLMs. Section~\ref{sec:architectures} reviews the core technical architectures and methodologies, including system configuration, external knowledge integration, and reasoning strategies. Section~\ref{sec:applications} discusses the diverse clinical and administrative application scenarios in which these agents are deployed. Section~\ref{sec:evaluation} outlines the evaluation frameworks and metrics used to assess their performance in healthcare settings. Finally, Section~\ref{sec:discussion} presents a discussion of the key challenges and future research directions for enhancing the reliability, safety, and clinical integration of LLM-based agents.
\fi

\section{Introduction}

Large Language Models (LLMs) are changing the field of artificial intelligence with their strong capabilities in text understanding, generation, and reasoning. The development of LLM-based agents has achieved notable success in many areas, from creative writing \cite{10.1145/3490099.3511105} to complex decision-making \cite{chai2025empowering,wei2023chainofthoughtpromptingelicitsreasoning}, which opens new opportunities for automating and enhancing human expertise. These agents have been applied in various fields by using the ability of LLMs to process and analyze complex information \cite{Wang_2024,cheng2024exploringlargelanguagemodel,xi2023risepotentiallargelanguage}.

In the medical domain, LLM-based agents have improved several clinical tasks. Recent work shows their use in diagnostic support \cite{kim2024mdagentsadaptivecollaborationllms}, patient communication \cite{mukherjee2024polarissafetyfocusedllmconstellation}, and medical education \cite{yu2024aipatientsimulatingpatientsehrs}. By combining LLMs with medical knowledge bases, clinical guidelines, and healthcare systems, these agents are designed to understand complex medical situations \cite{wei2024medaideomnimedicalaide}, offer evidence-based recommendations \cite{tang2024medagentslargelanguagemodels}, and support healthcare delivery \cite{mukherjee2024polarissafetyfocusedllmconstellation}. Despite these advances, the field still faces several challenges, including implementation issues \cite{sun2024applications}, safety concerns \cite{yuan-etal-2024-r}, and ethical considerations \cite{yan2025application}. Addressing these challenges is essential for the safe and reliable integration of LLM-based agents into clinical practice. Therefore, a comprehensive review is needed to analyze the current status and future directions of LLM-based agents in medicine.

In this article, we provide a systematic review of LLM-based agents in medicine, examining important research questions and future directions. We first discuss the architectures and methods, including system profile, external capacity enhancement, clinical planning, and medical reasoning in Section~\ref{sec:architectures}. Section~\ref{sec:applications} covers the various clinical and administrative application scenarios in which these agents are used. Section~\ref{sec:evaluation} outlines the evaluation frameworks and metrics for assessing their performance in healthcare settings. Finally, Section~\ref{sec:discussion} highlights key challenges and future research directions for improving the reliability, safety, and clinical integration of LLM-based agents. 

This review analyzed 60 studies on LLM-based medical agents published between 2022-2024, selected from major databases using healthcare AI-related keywords. The initial search yielded 300 papers, narrowed to 80 after screening, with 60 meeting final inclusion criteria.



% All papers included in this review were identified through a systematic search of major academic databases such as PubMed, ACM Digital Library, arXiv, and Google Scholar. Keywords such as "large language model", "medical agent", "clinical decision support", and "healthcare AI" were used to select relevant studies published between 2022 and 2024. This process initially identified about 300 articles, from which 80 were shortlisted based on title and abstract screening for relevance and quality. After a full-text review, approximately 60 studies specifically addressing LLM-based agents in medical contexts were selected for this survey.

\section{Background} \label{sec:background}

This section outlines the core differences between \textbf{LLMs} and \textbf{LLM-based agents} and highlights the unique considerations required for deploying such agents in medicine.

\subsection{LLM vs. LLM-based Agent}

An agent, as defined in AI, perceives its environment and takes actions accordingly \cite{russell2016artificial}. An LLM-based agent extends traditional LLMs by integrating external knowledge retrieval, task planning, and tool invocation, enabling structured decision-making in real-world applications \cite{xi2023risepotentiallargelanguage}. Unlike standard LLMs, which primarily process text, these agents operate autonomously and adapt dynamically to new information and tasks.

\subsection{Unique Considerations for LLM-based Agents in Medicine}

Deploying LLM-based agents in healthcare requires addressing several critical factors:

\noindent \textbf{Multimodal Integration}. Medical data spans text, imaging, and laboratory results. Agents must process and synthesize these inputs for accurate decision support \cite{electronics13112002}.

\noindent \textbf{Clinical Collaboration}. Healthcare relies on interdisciplinary work. Agents should facilitate information sharing and human-AI collaboration, ensuring physicians maintain oversight \cite{strong2024humanaicollaborationhealthcareguided}.

\noindent \textbf{Accuracy and Reliability}. Given the impact on patient outcomes, these agents must meet strict validation standards and minimize errors in diagnosis and treatment \cite{reddy2024generative}.

\noindent \textbf{Transparency and Traceability}. Clinical decisions must be auditable and explainable to align with medical ethics and regulatory requirements \cite{10.3389/frai.2022.879603}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{framework_diagram.png} 
    \caption{Conceptual framework of LLM-based medical agents. This figure depicts the architecture of the proposed LLM-based Medical Agent, consisting of system profile, external capacity enhancement, clinical palnning and medical reasoning. It supports four agent paradigms: \textbf{a) Single Agent}, \textbf{b) Sequential Task Chain}, \textbf{c) Collaborative Experts}, and \textbf{d) Iterative Evolution}. The framework integrates external tools and reasoning mechanisms to enable applications in medicine.
}
    \label{fig:framework_diagram}
    % \vspace{-2.5ex}
\end{figure*}

\section{LLM-based Medical Agent Architecture} \label{sec:architectures}

LLM-based agents in medicine require well-defined architectures to integrate complex clinical knowledge, facilitate medical decision-making, and ensure safe and effective deployment. This section presents a systematic overview of their architectural components, focusing on how these agents structure their operations to enhance clinical performance.

\subsection{Profile}

The profile of an agent plays a key role in defining and managing its role attributes, behavioral patterns, and operational competencies within medical systems, which traditionally involve information dissemination, resource distribution, and quality assurance. In medical applications, agent profiles follow three prototypes:

\noindent \textbf{Functional Modularization.}
This approach structures the agent system into specialized functional modules, each responsible for distinct tasks such as clinical data analysis or diagnostic reasoning. Systems like \textit{MEGDA} \cite{baniharouni2024magdamultiagentguidelinedrivendiagnostic} implement function-driven profiles where task assignments and workflows are explicitly defined to improve efficiency and adaptability.

\noindent \textbf{Role Specialization}
By mirroring real-world medical roles, this paradigm assigns agents to specific clinical functions, including diagnosis, medical imaging, treatment planning, and surgical assistance. These agents incorporate domain-specific knowledge and interact with healthcare systems for tasks such as imaging analysis and interdisciplinary coordination. In agent-driven operating room simulations \cite{wu2024surgboxagentdrivenoperatingroom}, LLM-based agents take on distinct medical roles to support clinical decision-making.

\noindent \textbf{Departmental Organization}
This framework structures agents based on medical disciplines, such as cardiology or hematology, establishing domain-specific knowledge boundaries. Agents rely on specialized disease knowledge graphs and dynamic collaboration mechanisms to facilitate interdisciplinary consultations. In multi-agent medical applications \cite{tang2024medagentslargelanguagemodels}, profiles are defined to reflect departmental expertise, improving coordination and decision-making in complex medical scenarios.

\subsection{Clinical Planning}

Effective clinical planning is at the core of LLM-based medical agents. The planning process breaks down complex medical tasks into smaller subtasks so that the system can interact with tools and databases specific to each clinical area \cite{Mehandru2024EvaluatingLL}. This division of tasks improves operational efficiency and aids in locating and correcting errors.

\noindent \textbf{Task Decomposition}
Clinical planning often follows a structured decomposition from high-level objectives to specific actions. A \textit{Single-Agent} model handles tasks autonomously, while a \textit{Sequential Task Chain} approach structures planning into distinct steps, such as data ingestion, hypothesis generation, treatment planning, and risk assessment. Each step interacts with specialized medical tools, ensuring task separation and facilitating precise error correction \cite{liu2024medchainbridginggapllm}.

\noindent \textbf{Multi-Agent Collaboration Across Departments}
For complex cases requiring interdisciplinary expertise, a \textit{Collaborative Experts} model assigns specialized agents to clinical areas such as radiology, pathology, and laboratory analysis. These agents communicate using standardized protocols to aggregate findings and refine diagnoses. This reduces diagnostic uncertainty by integrating insights from multiple specialties \cite{tang2024medagentslargelanguagemodels}.

\noindent \textbf{Adaptive Planning Architecture}
A dynamic \textit{Sequential Task Chain} or \textit{Collaborative Experts} framework adjusts decision-making based on real-time data and task complexity. For example, MDAgents framework \cite{kim2024mdagentsadaptivecollaborationllms} employs LLMs with predefined medical roles, which function autonomously or in coordination. Planning layer continuously updates clinical strategies, prioritizes urgent cases, and refines past decisions based on new evidence. Federated learning mechanisms further enhance adaptability by integrating diverse clinical experiences \cite{dutta2024adaptivereasoningactingmedical}.

\noindent \textbf{Iterative Self-Evolution}
Beyond static workflows, \textit{Iterative Evolution} frameworks enable continuous improvement. These systems maintain an experience base of past cases, refining decision-making over time. Self-improvement mechanisms allow agents to autonomously incorporate new medical data and learn from previous outcomes, progressively enhancing accuracy and reliability \cite{li2024agenthospitalsimulacrumhospital, du2024llmssimulatestandardizedpatients}.

\subsection{Medical Reasoning}

The Medical Reasoning module enhances diagnostic accuracy and transparency by structuring logical inference processes and integrating real-time feedback.

\noindent \textbf{Multi-Step Diagnostic Reasoning}
Complex cases are analyzed through sequential inference, where Chain-of-Thought methods \cite{wei2023chainofthoughtpromptingelicitsreasoning} generate step-by-step reasoning, and Tree-of-Thought approaches \cite{yao2023treethoughtsdeliberateproblem} explore multiple hypotheses in parallel, discarding less probable options. This structured approach improves diagnostic precision \cite{dutta2024adaptivereasoningactingmedical}.

\noindent \textbf{Reflective Decision-Making}
To handle clinical uncertainty, the system iteratively refines conclusions by incorporating real-time feedback and expert input. Inspired by the ReAct framework, it alternates between reasoning and action, identifying inconsistencies and improving decision robustness \cite{yao2023reactsynergizingreasoningacting,yue2024clinicalagentclinicaltrialmultiagent}.

\noindent \textbf{Collaborative Group Reasoning}
A multi-agent reasoning framework assigns specialized agents—such as primary care providers and specialists—to perform independent analyses. Their conclusions are aggregated through consensus mechanisms, mitigating biases and enhancing reliability \cite{zuo2025kg4diagnosishierarchicalmultiagentllm}.

\noindent \textbf{Memory-Enhanced Reasoning}
Integrating long-term memory modules enables agents to accumulate medical knowledge and past clinical experiences, refining decision-making over time. This persistent memory allows the system to adapt to new medical insights, improve reasoning capabilities, and maintain continuity in patient care \cite{li2024agenthospitalsimulacrumhospital}. Additionally, experience-based learning mechanisms enable LLM-based agents to update their diagnostic strategies dynamically, leading to more context-aware and personalized medical insights \cite{jiang2024longtermmemoryfoundation}.

\subsection{External Capacity Enhancement}

The external capacity enhancement augments the agent's capabilities by integrating it with real-world clinical data sources and specialized tools.

\noindent \textbf{Perception} subsystem processes diverse clinical inputs, including structured electronic health records (EHRs) to access patient histories and clinical parameters. Advanced Optical Character Recognition (OCR) techniques convert scanned documents into text, while models like CLIP analyze medical images, facilitating comprehensive multimodal understanding.

\noindent \textbf{Knowledge Integration} connects the agent with external sources such as medical knowledge graphs, drug interaction databases, and clinical guideline repositories. This connection helps the agent verify its inferences with trusted sources, thereby increasing both its accuracy and reliability \cite{li2024mmedagentlearningusemedical,huang2024benchmarkinglargelanguagemodels}.

\noindent \textbf{Action} layer allows agents to perform clinical tasks by using specialized tools such as medical calculators, electronic health record interfaces, and image analysis software. The system calls additional functionalities when processing complex data, ensuring that its outputs are complete and take the context into account \cite{shi2024ehragentcodeempowerslarge,zhu2024mentibridgingmedicalcalculator}.


\section{Application Scenarios} \label{sec:applications}

LLM-based agents are applied in various areas of medicine. This section outlines the main application scenarios and provides a summary in Table~\ref{tab:appliction}.\input{table1}

\subsection{Clinical Decision Support and Diagnosis}
In the area of Clinical Decision Support and Diagnosis, multi-agent frameworks based on LLMs improve clinical decision-making by addressing limitations of standalone LLMs. Systems in this area assign specialized roles to agents for intent recognition, diagnostic reasoning, and treatment planning so that healthcare delivery can be both personalized and sensitive to the context. For example, the framework proposed by Dutta and Hsiao \cite{dutta2024adaptivereasoningactingmedical} simulates interactions between doctors and patients to refine diagnostic reasoning and has shown better performance on datasets such as MedQA. The system developed by Ke et al. \cite{info:doi/10.2196/59439} reduces cognitive biases in diagnosis by using agents that provide expert opinions and critical evaluations. Other systems, such as \textit{MedAide} \cite{wei2024medaideomnimedicalaide}, coordinate agents across stages including pre-diagnosis, diagnosis, medication, and post-diagnosis, while frameworks such as MDagents \cite{kim2024demonstrationadaptivecollaborationlarge} and EHRagent \cite{tang2024medagentslargelanguagemodels} improve diagnostic accuracy through structured discussions and shared reasoning. Domain-specific applications also show promise. For instance, the work by Yue et al. \cite{yue2024clinicalagentclinicaltrialmultiagent} uses multi-agent collaboration to predict clinical trial outcomes by integrating large-scale domain knowledge. The Polaris framework \cite{mukherjee2024polarissafetyfocusedllmconstellation} combines general communication agents with task-specific agents to ensure safe patient interactions, and the system known as \textit{Rx Strategist} \cite{van2024rxstrategistprescriptionverification} uses knowledge graphs and multi-stage reasoning to check prescriptions for correct indications, dosages, and drug interactions.

\subsection{Clinical Data Analytics and Documentation}
In Clinical Data Analytics and Documentation, LLM-based agents show strong performance in processing both structured and unstructured data by using advanced architectures and retrieval-augmented generation techniques. The system \textit{ColaCare} proposed by Wang et al. \cite{wang2024colacareenhancingelectronichealth} integrates different agents to perform tasks such as mortality prediction and analysis of hospital readmission, demonstrating improved performance on the MIMIC-III and MIMIC-IV datasets. The work by Lee et al. \cite{lee2024improvingclinicaldocumentationai} introduces Sporo AI Scribe to address challenges related to the variability and complexity of clinical documentation. Research by Sudarshan \cite{sudarshan2024agenticllmworkflowsgenerating} shows that technical medical reports can be converted into patient-friendly formats by using iterative self-reflection and retrieval-augmented generation. In addition, \textit{Agent Hospital} \cite{li2024agenthospitalsimulacrumhospital} contributes to simulation systems by generating complete interactions that improve diagnostic and treatment capabilities.

\subsection{Medical Training and Simulation}
In Medical Training and Simulation, simulation environments are used to test and refine LLM-based agents before their use in clinical practice. Systems such as \textit{ClinicalLab} \cite{yan2024clinicallabaligningagentsmultidepartmental} and \textit{AI Hospital} \cite{fan2024aihospitalbenchmarkinglarge} evaluate diagnostic and treatment performance by simulating interactions across many specialties and complex healthcare scenarios. The system \textit{Agent Hospital} \cite{li2024agenthospitalsimulacrumhospital} further improves this process by allowing repeated training through large-scale simulations. In the field of medical education, systems such as \textit{MEDCO} \cite{wei2024medcomedicaleducationcopilots} support training in diagnostic reasoning and collaborative problem solving, while \textit{AIPatient} \cite{yu2024aipatientsimulatingpatientsehrs} integrates electronic health records with knowledge graphs to simulate realistic clinical scenarios. The system \textit{SurgBox} \cite{wu2024surgboxagentdrivenoperatingroom} provides a training environment for surgical procedures with real-time decision support that has been validated against actual surgical records.

\subsection{Healthcare Service Optimization}
In Healthcare Service Optimization, LLM-based agents improve the delivery of healthcare by automating tasks such as patient education, data collection, and support services. Research shows that automating these non-diagnostic tasks reduces the workload of healthcare professionals while maintaining service quality \cite{swarmsofLLM2024,mukherjee2024polarissafetyfocusedllmconstellation,laymouna2024roles}. There is also potential for the future automation of certain diagnostic tasks, including endoscopies and surgeries \cite{chadebecq2023artificial}. These implementations have resulted in measurable improvements in operational efficiency and patient satisfaction.


\section{Evaluation and Benchmarking} \label{sec:evaluation}

Evaluating LLM-based medical agents is essential for confirming their reliability, safety, and clinical effectiveness. A comprehensive evaluation framework is required to measure performance across different medical tasks, identify limitations, and guide improvements for clinical applications. A summary of the evaluation metrics and benchmark categories is provided in Table~\ref{tab:eval}.% in the appendix.

\input{table2}
\subsection{Benchmark Categories}

Benchmarks for LLM-based medical agents can be divided into three categories. 

\noindent\textbf{Static Question-Answering} benchmarks evaluate medical knowledge through tasks that have predetermined answers. For example, MedQA \cite{jin2020diseasedoespatienthave} simulates USMLE-style questions, MedMCQA \cite{pmlr-v174-pal22a} includes 194,000 questions covering 2,400 topics across 21 subjects, PubMedQA \cite{jin2019pubmedqa} assesses the understanding of biomedical research, and MMLU \cite{hendryckstest2021,hendrycks2021ethics} offers cross-domain single-choice questions. Although these datasets are useful for testing factual knowledge, they do not capture the interactive and sequential decision-making seen in clinical practice.

\noindent\textbf{Workflow-based Simulation} benchmarks mimic clinical decision-making through multiple stages. For instance, MedChain \cite{liu2024medchainbridginggapllm} contains 12,163 cases from 19 specialties and uses 7,338 medical images, AI Hospital \cite{fan2024aihospitalbenchmarkinglarge} evaluates interactions between healthcare providers and patients using the MVME dataset, AgentClinic \cite{schmidgall2024agentclinic} offers versions for both multimodal analysis and dialogue-based scenarios, and ClinicalLab \cite{yan2024clinicallabaligningagentsmultidepartmental} tests diagnostic performance across 24 departments and 150 diseases. These benchmarks reflect the dynamics of clinical reasoning and the adaptation required when patient information changes, although their complexity makes standardization challenging.

\noindent\textbf{Automated Evaluation} frameworks are developed to reduce reliance on human evaluators. For example, AI-SCE \cite{Mehandru2024EvaluatingLL} uses an OSCE-based framework for systematic evaluation, and RJUA-SPs \cite{10.1145/3637528.3671575} applies automated evaluation methods in urology using standardized patients and retrieval-augmented techniques.

\subsection{Metrics for Task-specific Evaluation}

\textbf{Exact Match Metrics} are used for tasks with clear correct answers, such as multiple-choice questions. In these tasks, accuracy, precision, and recall are calculated by directly comparing the model outputs with reference answers. Benchmarks such as MedQA \cite{jin2020diseasedoespatienthave} and MedMCQA \cite{pmlr-v174-pal22a} often use these metrics. While these metrics are effective for assessing factual knowledge, they may not be sufficient for tasks that involve complex reasoning or detailed explanations.

\noindent\textbf{Semantic Similarity Metrics} are applied to text generation tasks, such as writing clinical reports or diagnostic summaries. These metrics assess how well the meaning of the generated text matches that of the reference text. Metrics such as BLEU \cite{papineni-etal-2002-bleu}, which measures n-gram overlap, ROUGE \cite{lin-2004-rouge}, which evaluates summarization quality, and BertScore \cite{zhang2020bertscoreevaluatingtextgeneration}, which uses contextual embeddings to capture semantic relationships, have been applied in benchmarks such as ClinicalLab and MedChain.

\noindent\textbf{LLM-based Evaluation Metrics} use language models themselves to evaluate outputs based on factors such as coherence, relevance, and reasoning quality. For example, ChatCoach \cite{huang2024benchmarkinglargelanguagemodels} uses LLMs to assess the effectiveness of communication and decision making in patient consultations, while the Retrieval-Augmented Evaluation framework \cite{10.1145/3637528.3671575} used in RJUA-SPs measures the alignment of outputs with standard clinical pathways. This approach provides a scalable and adaptable method for assessing complex, multi-step clinical tasks.

\section{Discussions}
\label{sec:discussion}

Integrating Large Language Model (LLM)-based agents into medical workflows presents both challenges and opportunities. While previous work has achieved successes, the field remains in its early stages. Several significant challenges persist, and many opportunities require further exploration to fully realize their potential in healthcare applications. The following sections discuss these challenges and opportunities.

\subsection{Technical Challenges}

\subsubsection{Hallucination Management} LLM hallucinations—instances where models generate incorrect or misleading information—pose a significant risk in medical contexts, potentially leading to erroneous diagnoses and treatments \cite{Huang_2024}. Benchmarks such as \textit{MedHallBench} \cite{zuo2024medhallbench} and \textit{HaluEval} \cite{li2023halueval} highlight the need for reliable verification systems and error prevention mechanisms, especially in multi-agent scenarios where mistakes can propagate. Future research should focus on developing verification systems and dynamic error-correction methods that continuously update models with real-time, validated medical knowledge.

\subsubsection{Multimodal and Multilingual Integration} LLM-based agents must process various data types, including clinical texts and medical images, and handle variability in medical terminology across different languages and cultures \cite{li2024mmedagentlearningusemedical,Mehandru2024EvaluatingLL}. Variations in documentation standards and regional practices add to this complexity. It is crucial to develop models that can reliably operate in both multilingual and multimodal contexts.

\subsubsection{Cross-Department Integration} Healthcare environments encompass various departments, such as emergency, outpatient, and long-term care, each with its own workflows and documentation standards \cite{Qiu2024}. Achieving interoperability and accurate data exchange among these settings is challenging. Future work should focus on developing universal standards and adaptive interfaces that harmonize terminology and processes across departments, ensuring effective communication among LLM-based agents.

\subsection{Evaluation Challenges} Evaluating LLM-based medical agents is challenging. Traditional static benchmarks, which focus on fixed question-answering tasks, do not capture the dynamic and interactive aspects of clinical workflows, such as sequential decision-making, adaptive reasoning, and effective communication with patients and clinicians \cite{jin2020diseasedoespatienthave,schmidgall2024agentclinic}. Moreover, many medical applications require integrating heterogeneous data types, including text records, images, and laboratory results, which calls for evaluation frameworks that accurately simulate multimodal interactions \cite{liu2024medchainbridginggapllm,fan2024aihospitalbenchmarkinglarge}. Standard language metrics like BLEU \cite{papineni-etal-2002-bleu} and ROUGE \cite{lin-2004-rouge} assess only textual overlap and do not reflect clinical outcomes such as diagnostic accuracy. Additionally, dataset biases—such as the overrepresentation of specific conditions—can limit the generalizability of evaluation results across different healthcare settings \cite{yan2024clinicallabaligningagentsmultidepartmental}. Future research should develop integrated, multimodal evaluation frameworks that combine quantitative measures with qualitative clinical assessments and establish standardized clinical performance metrics while reducing dataset biases \cite{Mehandru2024EvaluatingLL}.

\subsection{Implementation Barriers}

%\subsubsection{Data Constraints}
%Medical data is often restricted by privacy regulations such as HIPAA, GDPR, and institutional policies, unlike open internet datasets. This limitation reduces data sharing and may lead to model biases and incomplete learning if models are trained on small or non-representative datasets.
% Somewhat overlap with 6.4.3, reduced for paragraph leangth consideration

\subsubsection{System Integration Complexity}
Large-scale systems like the Polaris healthcare system \cite{mukherjee2024polarissafetyfocusedllmconstellation}, which involve millions of professionals and established decision-making processes, illustrate the complexity of integration. Although many LLM frameworks prove valuable in specific applications, their broader integration does not always lead to improvements in operational efficiency.

\subsubsection{Resource Allocation Dilemma}
Developing and maintaining LLM-based agents requires significant computational resources, resulting in high costs for medical institutions. Such investments may produce systems that are not entirely reliable, raising concerns about their cost-effectiveness.

\subsection{Ethical and Privacy Concerns}
\subsubsection{Patient-Centered Design}
Medical diagnosis systems powered by LLM agents currently receive limited feedback from patients and caregivers, despite the importance of including their perspectives in decision-making \cite{kim2024mdagents}. Most existing frameworks focus solely on interactions with physicians. A more responsible approach would integrate patient narratives, physician observations, and caregiver input to support a truly patient-centered process.

\subsubsection{Algorithmic Bias}
Both general-purpose and medically fine-tuned LLMs can exhibit various biases, including social and cognitive biases. The \textit{BiasMedQA} benchmark \cite{schmidgall2024addressing} evaluated seven types of bias in state-of-the-art medical LLMs and found that precision can fall below 80\%, with some models performing as poorly as 50\%. This raises concerns about the reliability of these models in addressing bias. Medical agents must be designed to make responsible decisions, and reducing bias is essential for achieving this goal.

\subsubsection{Privacy and Security Threats}
Sensitive data used for training may be exposed during text generation or extracted through techniques such as inference attacks \cite{kandpal2023user} or data extraction \cite{carlini2021extracting}. It is critical to protect sensitive information in accordance with regulations like GDPR (EU) \cite{gdpr2016general} and HIPAA (USA) \cite{act1996health} when deploying medical agents. Data collection for developing LLM agents must prioritize privacy protection. \citep{dou2024exploringllmbaseddataannotation} suggests using LLMs for autonomous data generation and labeling as a means to protect privacy. In addition, privacy-preserving data processing methods, such as differential privacy, can add controlled noise to data so that individual records do not significantly influence overall results while preserving data utility.

\subsection{Future Opportunities and Application}

\subsubsection{Inspiration of O1 and R1 for Medical Reasoning}
The evolution of LLM-based medical agents can benefit from insights drawn from DeepSeek R1 and inference-time scaling strategies. DeepSeek R1 \cite{guo2025deepseek} has shown that reinforcement learning combined with long-chain reasoning leads to more accurate and context-aware medical decision-making, offering potentials for improving autonomous medical agents \cite{faray2025does}. By continuously optimizing AI-generated diagnoses and treatment recommendations through iterative self-evolution, LLM-based agents can better integrate multimodal clinical data, including electronic health records, medical images, and laboratory findings \cite{xu2024nextgenerationmedicalagento1}. Inference-time scaling, allowing LLMs more reasoning time, has been shown to improve performance in complex tasks such as differential diagnosis and treatment planning \cite{huang2025o1replicationjourney}, consistent with the hypothetico-deductive method used in clinical reasoning. Future research should explore how LLM-based agents can dynamically adjust inference time based on task complexity while incorporating reinforcement learning-based optimization techniques to enhance adaptability and reliability in clinical settings.

\subsubsection{Integration with Physical Systems}
Expanding LLM-based agents from virtual applications to integration with physical systems represents a significant step in medical care. While LLMs excel at data analysis and decision support, connecting them with physical systems such as medical robots could enable direct patient care. Such systems might combine language processing with physical inputs to support tasks like surgical assistance and patient monitoring. For example, empowering nursing robots \cite{nursingrobot} is one potential approach. However, this integration raises challenges regarding safety and real-time performance. Addressing technical limitations, ensuring system reliability, and resolving ethical concerns are necessary for successful integration. Hardware systems must accurately execute LLM outputs because errors could endanger patient safety, and high costs or technical complexity may limit system availability. Future work should focus on improving the integration of LLM-based agents with physical systems and on creating practical implementation frameworks.

\subsubsection{Advancements in Training Simulation}
Current medical LLM agents often use simulated hospitals for training, such as the \textit{Agent Hospital} framework, which enables the autonomous evolution of doctor agents through synthetic patient interactions \cite{li2024agenthospitalsimulacrumhospital}. Extending these simulations to include educational medical games could improve training data generation and learning experiences, even though challenges in data quality remain. AI-driven patient simulations that provide structured feedback have demonstrated effectiveness in enhancing clinical decision-making \cite{Brgge2024LargeLM}, but validating data generated by such games remains resource intensive.

\section{Conclusion}

This survey examines LLM-based agents in medicine, covering their architectures, applications, and challenges. While these agents enhance diagnostics, data analysis, and clinical workflows, issues remain in hallucination management, multimodal integration, and medical reasoning accuracy. Future work should focus on real-time error correction, improved multimodal fusion, and hybrid reasoning to enhance reliability and clinical utility.


\section*{Limitations}
This survey has several inherent limitations that should be considered. Due to the rapid development of LLM-based medical agents, our review primarily covers works published between 2022 and early 2024, which means future developments may introduce new architectures and approaches not captured in this analysis. Additionally, while we aimed for comprehensive coverage, we focused mainly on English-language publications in major academic databases such as PubMed, ACM Digital Library, arXiv, and Google Scholar. Valuable work published in other languages or regional databases may not be included in our analysis. These limitations reflect the inherent constraints of conducting a survey in a rapidly evolving field rather than shortcomings in the reviewed research itself.


%  This paper has several limitations. It focuses on studies published between 2022 and 2024 and does not cover earlier work or all available literature in the field. The survey relies on a review of existing publications and does not include experimental evaluations or a detailed quantitative analysis of LLM-based agents in real clinical settings. In addition, the paper does not compare model performance across different clinical applications in depth. Future work should expand literature coverage and include experimental validation to provide a more complete assessment of LLM-based agents in medicine.

% \section*{Ethical Statement}

% There are no ethical issues.


\bibliography{custom}

\appendix

\section{Appendix}
\label{sec:appendix}
\input{table2}

\subsection{Paper Collection}
All papers included in this review were identified through a systematic search of major academic databases such as PubMed, ACM Digital Library, arXiv, and Google Scholar. Keywords such as "large language model", "medical agent", "clinical decision support", and "healthcare AI" were used to select relevant studies published between 2022 and 2024. This process initially identified about 300 articles, from which 80 were shortlisted based on title and abstract screening for relevance and quality. After a full-text review, approximately 60 studies specifically addressing LLM-based agents in medical contexts were selected for this survey.

\subsection{Evaluation and Benchmarking}
Table~\ref{tab:eval} demonstrates common evaluation benchmarks and metrics for the llm-based agents in medicine.
\end{document}
